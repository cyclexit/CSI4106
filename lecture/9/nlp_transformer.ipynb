{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is taken from code examples in:\n",
    "https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "\n",
    "The code is modified according to the lecture on Introduction to AI (CSI4106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        \n",
    "        super(TransformerLayer, self).__init__()\n",
    "        \n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        \n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    # the computation\n",
    "    def call(self, inputs, training):\n",
    "        \n",
    "        attn_output = self.att(inputs, inputs)\n",
    "\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        # inputs + attention -> Residual connection\n",
    "        # layernorm -> normalizes the residual output\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        # further processing through a set of dense layers with relu activation\n",
    "        ffn_output = self.ffn(out1)\n",
    "\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "\n",
    "        # output from the dense layers combined with the previous normalized input\n",
    "        # combined input is further normalized\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        \n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6524, 4)\n",
      "(2797, 4)\n",
      "4    0.356683\n",
      "5    0.325414\n",
      "3    0.148835\n",
      "2    0.093961\n",
      "1    0.075107\n",
      "Name: stars, dtype: float64\n",
      "4    0.356811\n",
      "5    0.325349\n",
      "3    0.148731\n",
      "2    0.094029\n",
      "1    0.075080\n",
      "Name: stars, dtype: float64\n",
      "Counter({4: 2327, 5: 2123, 3: 971, 2: 613, 1: 490})\n",
      "Counter({4: 998, 5: 910, 3: 416, 2: 263, 1: 210})\n",
      "Counter({3: 2327, 4: 2123, 2: 971, 1: 613, 0: 490})\n",
      "Counter({3: 998, 4: 910, 2: 416, 1: 263, 0: 210})\n"
     ]
    }
   ],
   "source": [
    "# split the data\n",
    "df = pd.read_csv('./data/yelp_cleaned.csv')\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(df, df['stars']):\n",
    "    df_train = df.loc[train_index]\n",
    "    df_test = df.loc[test_index]\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_train.stars.value_counts()/len(df_train))\n",
    "print(df_test.stars.value_counts()/len(df_test))\n",
    "\n",
    "train_x = df_train.clean_text.to_numpy()\n",
    "train_y = df_train.stars.to_numpy()\n",
    "test_x = df_test.clean_text.to_numpy()\n",
    "test_y = df_test.stars.to_numpy()\n",
    "\n",
    "print(Counter(train_y))\n",
    "print(Counter(test_y))\n",
    "\n",
    "lbl_encoder = LabelEncoder()\n",
    "train_y = lbl_encoder.fit_transform(train_y)\n",
    "test_y = lbl_encoder.transform(test_y)\n",
    "\n",
    "print(Counter(train_y))\n",
    "print(Counter(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "# token embedding size\n",
    "embed_dim = 256\n",
    "# number of attention heads\n",
    "num_heads = 2\n",
    "# hidden layer size\n",
    "ff_dim = 32\n",
    "# maximum length for text\n",
    "maxlen = 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23718\n",
      "(6524, 450)\n",
      "(6524,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='[UNK]')\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# print(word_index)\n",
    "print(len(word_index))\n",
    "\n",
    "# converting train data to a sequence\n",
    "train_x_seq = tokenizer.texts_to_sequences(train_x)\n",
    "# print(train_x_seq)\n",
    "train_x_pad = pad_sequences(train_x_seq, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# print(len(train_x_seq[0]))\n",
    "# print(len(train_x_pad[0]))\n",
    "\n",
    "# print(train_x_seq[0])\n",
    "# print(train_x_pad[0])\n",
    "\n",
    "# converting test data to a sequence\n",
    "test_x_seq = tokenizer.texts_to_sequences(test_x)\n",
    "# print(train_x_seq)\n",
    "test_x_pad = pad_sequences(test_x_seq, maxlen=maxlen, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "\n",
    "print(train_x_pad.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(maxlen,))\n",
    "# embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerLayer(embed_dim, num_heads, ff_dim)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "143/143 [==============================] - 9s 44ms/step - loss: 1.4976 - accuracy: 0.3557 - val_loss: 1.4387 - val_accuracy: 0.3483\n",
      "Epoch 2/2\n",
      "143/143 [==============================] - 6s 43ms/step - loss: 1.3744 - accuracy: 0.3673 - val_loss: 1.3174 - val_accuracy: 0.3575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fc6b328490>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    train_x_pad, train_y, batch_size=32, epochs=2, validation_split=0.3\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
