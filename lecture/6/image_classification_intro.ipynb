{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 28900), started 21:50:39 ago. (Use '!kill 28900' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "import datetime\n",
    "import os\n",
    "# for reproducibility purposes\n",
    "np.random.seed(42)\n",
    "python_random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# load tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    d = datetime.datetime.today()\n",
    "    timestamp = d.strftime('%Y%m%d_%H%M%S')\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(os.path.curdir, 'logs', timestamp)\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(os.path.curdir, 'models', timestamp)\n",
    "\n",
    "    os.mkdir(tensorlog_folder)\n",
    "    os.mkdir(checkpoint_folder)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 0: T-shirt/top, 1: Trouser, 2: Pullover, 3: Dress, 4: Coat, 5: Sandal, 6: Shirt, 7: Sneaker, 8: Bag, 9: Ankle boot\n",
    "data = datasets.fashion_mnist\n",
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = data.load_data()\n",
    "\n",
    "print(train_imgs.shape)\n",
    "print(test_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({9: 6000, 0: 6000, 3: 6000, 2: 6000, 7: 6000, 5: 6000, 1: 6000, 6: 6000, 4: 6000, 8: 6000})\n",
      "Counter({9: 1000, 2: 1000, 1: 1000, 6: 1000, 4: 1000, 5: 1000, 7: 1000, 3: 1000, 8: 1000, 0: 1000})\n"
     ]
    }
   ],
   "source": [
    "# normalize the image data\n",
    "train_imgs = train_imgs.astype('float32') / 255\n",
    "test_imgs = test_imgs.astype('float32') / 255\n",
    "\n",
    "print(Counter(train_lbls))\n",
    "print(Counter(test_lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print an image\n",
    "print(train_lbls[9])\n",
    "image = train_imgs[9]\n",
    "# print(type(image), image.shape, image)\n",
    "plt.imshow(image, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "657/657 [==============================] - 4s 4ms/step - loss: 0.5740 - accuracy: 0.7994 - val_loss: 0.5337 - val_accuracy: 0.8122\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184156\\assets\n",
      "Epoch 2/10\n",
      "657/657 [==============================] - 2s 4ms/step - loss: 0.4139 - accuracy: 0.8517 - val_loss: 0.3927 - val_accuracy: 0.8573\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184156\\assets\n",
      "Epoch 3/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.3676 - accuracy: 0.8661 - val_loss: 0.4549 - val_accuracy: 0.8362\n",
      "Epoch 4/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.3381 - accuracy: 0.8763 - val_loss: 0.4159 - val_accuracy: 0.8551\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 5/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2826 - accuracy: 0.8981 - val_loss: 0.3271 - val_accuracy: 0.8849\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184156\\assets\n",
      "Epoch 6/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2738 - accuracy: 0.9018 - val_loss: 0.3240 - val_accuracy: 0.8857\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184156\\assets\n",
      "Epoch 7/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2703 - accuracy: 0.9020 - val_loss: 0.3214 - val_accuracy: 0.8862\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184156\\assets\n",
      "Epoch 8/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2673 - accuracy: 0.9036 - val_loss: 0.3220 - val_accuracy: 0.8848\n",
      "Epoch 9/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2642 - accuracy: 0.9040 - val_loss: 0.3180 - val_accuracy: 0.8872\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184156\\assets\n",
      "Epoch 10/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2617 - accuracy: 0.9054 - val_loss: 0.3192 - val_accuracy: 0.8878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2983b439fa0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the keras sequential model\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu', name='dense_1'),\n",
    "    layers.Dense(10, activation='softmax', name='dense_out')\n",
    "])\n",
    "\n",
    "# print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get the required directories\n",
    "check_dir, tboard_dir = make_directories()\n",
    "\n",
    "# using the callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training data with a validation split\n",
    "# by default .fit will shuffle the data\n",
    "model.fit(\n",
    "    train_imgs, \n",
    "    train_lbls, \n",
    "    validation_split=0.3, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    callbacks=[reduce_lr, early_stop, checkpoints, tensorboard]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3473 - accuracy: 0.8796\n",
      "[0.3473190367221832, 0.8795999884605408]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84      1000\n",
      "           1       0.99      0.97      0.98      1000\n",
      "           2       0.77      0.83      0.79      1000\n",
      "           3       0.86      0.91      0.88      1000\n",
      "           4       0.80      0.79      0.79      1000\n",
      "           5       0.97      0.96      0.96      1000\n",
      "           6       0.72      0.64      0.68      1000\n",
      "           7       0.92      0.96      0.94      1000\n",
      "           8       0.96      0.97      0.97      1000\n",
      "           9       0.97      0.95      0.96      1000\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test data\n",
    "test_evals = model.evaluate(test_imgs, test_lbls)\n",
    "print(test_evals)\n",
    "# predict on the test data\n",
    "predictions = model.predict(test_imgs)\n",
    "predicted_lbls = np.argmax(predictions, axis=1)\n",
    "cr = classification_report(test_lbls, predicted_lbls)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_out (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "657/657 [==============================] - 3s 4ms/step - loss: 0.5631 - accuracy: 0.8013 - val_loss: 0.5031 - val_accuracy: 0.8259\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184223\\assets\n",
      "Epoch 2/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.4061 - accuracy: 0.8541 - val_loss: 0.3931 - val_accuracy: 0.8572\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184223\\assets\n",
      "Epoch 3/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.3612 - accuracy: 0.8687 - val_loss: 0.4634 - val_accuracy: 0.8312\n",
      "Epoch 4/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.3337 - accuracy: 0.8777 - val_loss: 0.4213 - val_accuracy: 0.8549\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 5/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2793 - accuracy: 0.8987 - val_loss: 0.3247 - val_accuracy: 0.8844\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184223\\assets\n",
      "Epoch 6/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2709 - accuracy: 0.9016 - val_loss: 0.3226 - val_accuracy: 0.8851\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184223\\assets\n",
      "Epoch 7/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2676 - accuracy: 0.9028 - val_loss: 0.3190 - val_accuracy: 0.8853\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184223\\assets\n",
      "Epoch 8/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2648 - accuracy: 0.9036 - val_loss: 0.3206 - val_accuracy: 0.8856\n",
      "Epoch 9/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2616 - accuracy: 0.9048 - val_loss: 0.3177 - val_accuracy: 0.8880\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184223\\assets\n",
      "Epoch 10/10\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.2594 - accuracy: 0.9058 - val_loss: 0.3186 - val_accuracy: 0.8863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2985a3b92b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limitations with the sequential model\n",
    "# single input and output\n",
    "# sequence of layers\n",
    "\n",
    "# using the functional API\n",
    "# can be used with multiple inputs and outputs\n",
    "\n",
    "# using the keras functional API\n",
    "inputs = keras.Input(shape=(28,28))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(128, activation='relu', name='dense_1')(x)\n",
    "\n",
    "outputs = layers.Dense(10, activation='softmax', name='dense_out')(x)\n",
    "\n",
    "# creating the Model by grouping the layers\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get the required directories\n",
    "check_dir, tboard_dir = make_directories()\n",
    "\n",
    "# using the callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training data with a validation split\n",
    "# by default .fit will shuffle the data\n",
    "model.fit(\n",
    "    train_imgs, \n",
    "    train_lbls, \n",
    "    validation_split=0.3, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    callbacks=[reduce_lr, early_stop, checkpoints, tensorboard]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3461 - accuracy: 0.8791\n",
      "[0.34606388211250305, 0.8791000247001648]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83      1000\n",
      "           1       0.99      0.97      0.98      1000\n",
      "           2       0.77      0.82      0.79      1000\n",
      "           3       0.85      0.90      0.88      1000\n",
      "           4       0.79      0.80      0.79      1000\n",
      "           5       0.96      0.96      0.96      1000\n",
      "           6       0.74      0.63      0.68      1000\n",
      "           7       0.92      0.96      0.94      1000\n",
      "           8       0.96      0.98      0.97      1000\n",
      "           9       0.97      0.94      0.96      1000\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test data\n",
    "test_evals = model.evaluate(test_imgs, test_lbls)\n",
    "print(test_evals)\n",
    "# predict on the test data\n",
    "predictions = model.predict(test_imgs)\n",
    "predicted_lbls = np.argmax(predictions, axis=1)\n",
    "cr = classification_report(test_lbls, predicted_lbls)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fashion_mnist\n",
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = data.load_data()\n",
    "\n",
    "# reshape and normalize data for CNN\n",
    "train_imgs = train_imgs.reshape((60000, 28, 28, 1)) \n",
    "train_imgs = train_imgs.astype(\"float32\") / 255 \n",
    "test_imgs = test_imgs.reshape((10000, 28, 28, 1)) \n",
    "test_imgs = test_imgs.astype(\"float32\") / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                11530     \n",
      "=================================================================\n",
      "Total params: 104,202\n",
      "Trainable params: 104,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "657/657 [==============================] - 10s 7ms/step - loss: 0.5644 - accuracy: 0.7923 - val_loss: 0.4432 - val_accuracy: 0.8314\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184249\\assets\n",
      "Epoch 2/10\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.3547 - accuracy: 0.8709 - val_loss: 0.3415 - val_accuracy: 0.8744\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184249\\assets\n",
      "Epoch 3/10\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.3017 - accuracy: 0.8900 - val_loss: 0.3651 - val_accuracy: 0.8659\n",
      "Epoch 4/10\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.2704 - accuracy: 0.9009 - val_loss: 0.3006 - val_accuracy: 0.8919\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184249\\assets\n",
      "Epoch 5/10\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.2449 - accuracy: 0.9098 - val_loss: 0.2947 - val_accuracy: 0.8934\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184249\\assets\n",
      "Epoch 6/10\n",
      "657/657 [==============================] - 4s 5ms/step - loss: 0.2229 - accuracy: 0.9185 - val_loss: 0.3147 - val_accuracy: 0.8845\n",
      "Epoch 7/10\n",
      "657/657 [==============================] - 4s 5ms/step - loss: 0.2066 - accuracy: 0.9236 - val_loss: 0.2672 - val_accuracy: 0.9054\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184249\\assets\n",
      "Epoch 8/10\n",
      "657/657 [==============================] - 4s 6ms/step - loss: 0.1897 - accuracy: 0.9298 - val_loss: 0.2944 - val_accuracy: 0.8942\n",
      "Epoch 9/10\n",
      "657/657 [==============================] - 3s 5ms/step - loss: 0.1752 - accuracy: 0.9349 - val_loss: 0.2921 - val_accuracy: 0.9024\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 10/10\n",
      "657/657 [==============================] - 4s 5ms/step - loss: 0.1273 - accuracy: 0.9555 - val_loss: 0.2493 - val_accuracy: 0.9177\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_184249\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x295ab70b130>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(28, 28, 1)) \n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs) \n",
    "x = layers.MaxPooling2D(pool_size=2)(x) \n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x) \n",
    "x = layers.MaxPooling2D(pool_size=2)(x) \n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x) \n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x) \n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get the required directories\n",
    "check_dir, tboard_dir = make_directories()\n",
    "\n",
    "# using the callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training data with a validation split\n",
    "# by default .fit will shuffle the data\n",
    "model.fit(\n",
    "    train_imgs, \n",
    "    train_lbls, \n",
    "    validation_split=0.3, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    callbacks=[reduce_lr, early_stop, checkpoints, tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2632 - accuracy: 0.9127\n",
      "[0.26317542791366577, 0.9126999974250793]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86      1000\n",
      "           1       0.99      0.98      0.99      1000\n",
      "           2       0.85      0.87      0.86      1000\n",
      "           3       0.92      0.92      0.92      1000\n",
      "           4       0.86      0.87      0.86      1000\n",
      "           5       0.99      0.98      0.98      1000\n",
      "           6       0.75      0.73      0.74      1000\n",
      "           7       0.95      0.98      0.96      1000\n",
      "           8       0.98      0.97      0.98      1000\n",
      "           9       0.98      0.96      0.97      1000\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test data\n",
    "test_evals = model.evaluate(test_imgs, test_lbls)\n",
    "print(test_evals)\n",
    "# predict on the test data\n",
    "predictions = model.predict(test_imgs)\n",
    "predicted_lbls = np.argmax(predictions, axis=1)\n",
    "cr = classification_report(test_lbls, predicted_lbls)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use \"image_dataset_from_directory\" to read, shuffle, resize, batch, etc.. to load images\n",
    "* In case the model overfits, use different data augmentation strategies such as: RandomFlip, RandomRotation, RandomZoom\n",
    "* In case your dataset is small, use a pretrained model (i.e., if the model is well generalized)\n",
    "    * e.g., ImageNet, ResNet"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
