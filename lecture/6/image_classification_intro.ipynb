{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "import datetime\n",
    "import os\n",
    "# for reproducibility purposes\n",
    "np.random.seed(42)\n",
    "python_random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# load tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    d = datetime.datetime.today()\n",
    "    timestamp = d.strftime('%Y%m%d_%H%M%S')\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(os.path.curdir, 'logs', timestamp)\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(os.path.curdir, 'models', timestamp)\n",
    "\n",
    "    os.mkdir(tensorlog_folder)\n",
    "    os.mkdir(checkpoint_folder)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: T-shirt/top, 1: Trouser, 2: Pullover, 3: Dress, 4: Coat, 5: Sandal, 6: Shirt, 7: Sneaker, 8: Bag, 9: Ankle boot\n",
    "data = datasets.fashion_mnist\n",
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = data.load_data()\n",
    "\n",
    "print(train_imgs.shape)\n",
    "print(test_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the image data\n",
    "train_imgs = train_imgs.astype('float32') / 255\n",
    "test_imgs = test_imgs.astype('float32') / 255\n",
    "\n",
    "print(Counter(train_lbls))\n",
    "print(Counter(test_lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print an image\n",
    "print(train_lbls[9])\n",
    "image = train_imgs[9]\n",
    "plt.imshow(image, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the keras sequential model\n",
    "model = keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu', name='dense_1'),\n",
    "    layers.Dense(10, activation='softmax', name='dense_out')\n",
    "])\n",
    "\n",
    "# print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get the required directories\n",
    "check_dir, tboard_dir = make_directories()\n",
    "\n",
    "# using the callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training data with a validation split\n",
    "# by default .fit will shuffle the data\n",
    "model.fit(\n",
    "    train_imgs, \n",
    "    train_lbls, \n",
    "    validation_split=0.3, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    callbacks=[reduce_lr, early_stop, checkpoints, tensorboard]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test data\n",
    "test_evals = model.evaluate(test_imgs, test_lbls)\n",
    "print(test_evals)\n",
    "# predict on the test data\n",
    "predictions = model.predict(test_imgs)\n",
    "predicted_lbls = np.argmax(predictions, axis=1)\n",
    "cr = classification_report(test_lbls, predicted_lbls)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limitations with the sequential model\n",
    "# single input and output\n",
    "# sequence of layers\n",
    "\n",
    "# using the functional API\n",
    "# can be used with multiple inputs and outputs\n",
    "\n",
    "# using the keras functional API\n",
    "inputs = keras.Input(shape=(28,28))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(128, activation='relu', name='dense_1')(x)\n",
    "\n",
    "outputs = layers.Dense(10, activation='softmax', name='dense_out')(x)\n",
    "\n",
    "# creating the Model by grouping the layers\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# print the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get the required directories\n",
    "check_dir, tboard_dir = make_directories()\n",
    "\n",
    "# using the callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training data with a validation split\n",
    "# by default .fit will shuffle the data\n",
    "model.fit(\n",
    "    train_imgs, \n",
    "    train_lbls, \n",
    "    validation_split=0.3, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    callbacks=[reduce_lr, early_stop, checkpoints, tensorboard]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test data\n",
    "test_evals = model.evaluate(test_imgs, test_lbls)\n",
    "print(test_evals)\n",
    "# predict on the test data\n",
    "predictions = model.predict(test_imgs)\n",
    "predicted_lbls = np.argmax(predictions, axis=1)\n",
    "cr = classification_report(test_lbls, predicted_lbls)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fashion_mnist\n",
    "(train_imgs, train_lbls), (test_imgs, test_lbls) = data.load_data()\n",
    "\n",
    "# reshape and normalize data for CNN\n",
    "train_imgs = train_imgs.reshape((60000, 28, 28, 1)) \n",
    "train_imgs = train_imgs.astype(\"float32\") / 255 \n",
    "test_imgs = test_imgs.reshape((10000, 28, 28, 1)) \n",
    "test_imgs = test_imgs.astype(\"float32\") / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(28, 28, 1)) \n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs) \n",
    "x = layers.MaxPooling2D(pool_size=2)(x) \n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x) \n",
    "x = layers.MaxPooling2D(pool_size=2)(x) \n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x) \n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x) \n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# get the required directories\n",
    "check_dir, tboard_dir = make_directories()\n",
    "\n",
    "# using the callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=tboard_dir\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training data with a validation split\n",
    "# by default .fit will shuffle the data\n",
    "model.fit(\n",
    "    train_imgs, \n",
    "    train_lbls, \n",
    "    validation_split=0.3, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    callbacks=[reduce_lr, early_stop, checkpoints, tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test data\n",
    "test_evals = model.evaluate(test_imgs, test_lbls)\n",
    "print(test_evals)\n",
    "# predict on the test data\n",
    "predictions = model.predict(test_imgs)\n",
    "predicted_lbls = np.argmax(predictions, axis=1)\n",
    "cr = classification_report(test_lbls, predicted_lbls)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use \"image_dataset_from_directory\" to read, shuffle, resize, batch, etc.. to load images\n",
    "* In case the model overfits, use different data augmentation strategies such as: RandomFlip, RandomRotation, RandomZoom\n",
    "* In case your dataset is small, use a pretrained model (i.e., if the model is well generalized)\n",
    "    * e.g., ImageNet, ResNet"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
