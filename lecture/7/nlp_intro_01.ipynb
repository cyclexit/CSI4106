{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentAnalyzer, SentimentIntensityAnalyzer\n",
    "\n",
    "from nlpretext import Preprocessor\n",
    "from nlpretext.basic.preprocess import normalize_whitespace, remove_punct, remove_eol_characters, remove_stopwords, \\\n",
    "    lower_text, remove_accents, remove_multiple_spaces_and_strip_text, replace_numbers, replace_emails, replace_urls\n",
    "from nlpretext.social.preprocess import remove_mentions, remove_hashtag, remove_emoji\n",
    "\n",
    "# some useful libraries\n",
    "# spacy\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        but      cold       day        is    really     sunny     today  \\\n",
      "0  0.000000  0.392053  0.614226  0.484263  0.000000  0.000000  0.484263   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.577350  0.000000   \n",
      "2  0.000000  0.392053  0.000000  0.484263  0.614226  0.000000  0.484263   \n",
      "3  0.552816  0.352855  0.000000  0.000000  0.000000  0.435847  0.000000   \n",
      "\n",
      "        was  yesterday  \n",
      "0  0.000000   0.000000  \n",
      "1  0.577350   0.577350  \n",
      "2  0.000000   0.000000  \n",
      "3  0.435847   0.435847  \n",
      "['but' 'cold' 'day' 'is' 'really' 'sunny' 'today' 'was' 'yesterday']\n",
      "['but' 'cold' 'day' 'is' 'really' 'sunny' 'today' 'was' 'yesterday']\n",
      "{'today': 6, 'is': 3, 'cold': 1, 'day': 2, 'yesterday': 8, 'was': 7, 'sunny': 5, 'really': 4, 'but': 0}\n"
     ]
    }
   ],
   "source": [
    "# data representation\n",
    "text = ['today is a cold day', \n",
    "        'yesterday was sunny', \n",
    "        'today is really cold', \n",
    "        'yesterday was sunny but cold']\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range=(1, 1))\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "doc_term_matrix = count_vect.fit_transform(text) \n",
    "df_count = pd.DataFrame(data=doc_term_matrix.toarray(), columns=count_vect.get_feature_names_out())\n",
    "\n",
    "doc_term_matrix = tfidf_vect.fit_transform(text)\n",
    "df_tfidf = pd.DataFrame(data=doc_term_matrix.toarray(), columns=tfidf_vect.get_feature_names_out())\n",
    "print(df_tfidf)\n",
    "\n",
    "print(count_vect.get_feature_names_out())\n",
    "print(tfidf_vect.get_feature_names_out())\n",
    "\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today cold day\n"
     ]
    }
   ],
   "source": [
    "# text pre-processing\n",
    "def clean_text(text):\n",
    "    preprocessor = Preprocessor()\n",
    "    preprocessor.pipe(lower_text)\n",
    "    preprocessor.pipe(remove_mentions)\n",
    "    preprocessor.pipe(remove_hashtag)\n",
    "    preprocessor.pipe(remove_emoji)\n",
    "    preprocessor.pipe(remove_eol_characters)\n",
    "    preprocessor.pipe(remove_stopwords, args={'lang': 'en'})\n",
    "    preprocessor.pipe(remove_punct)\n",
    "    preprocessor.pipe(replace_urls)\n",
    "    preprocessor.pipe(replace_emails)\n",
    "    preprocessor.pipe(replace_numbers)\n",
    "    preprocessor.pipe(remove_accents)\n",
    "    preprocessor.pipe(remove_multiple_spaces_and_strip_text)\n",
    "    preprocessor.pipe(normalize_whitespace)\n",
    "\n",
    "    text = preprocessor.run(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "text = ['today is a cold day.', \n",
    "        'was it sunny?', \n",
    "        'today is.... really cold', \n",
    "        'yesterday was sunny but cold :-(']\n",
    "        \n",
    "cleaned_text = clean_text(text[0])\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n",
      "Index(['business_id', 'date', 'review_id', 'stars', 'text', 'type', 'user_id',\n",
      "       'cool', 'useful', 'funny'],\n",
      "      dtype='object')\n",
      "              business_id        date               review_id  stars  \\\n",
      "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
      "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
      "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
      "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
      "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
      "\n",
      "                                                text    type  \\\n",
      "0  My wife took me here on my birthday for breakf...  review   \n",
      "1  I have no idea why some people give bad review...  review   \n",
      "2  love the gyro plate. Rice is so good and I als...  review   \n",
      "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
      "4  General Manager Scott Petello is a good egg!!!...  review   \n",
      "\n",
      "                  user_id  cool  useful  funny  \n",
      "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
      "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
      "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
      "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
      "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  \n",
      "4    3526\n",
      "5    3337\n",
      "3    1461\n",
      "2     927\n",
      "1     749\n",
      "Name: stars, dtype: int64\n",
      "9998\n",
      "                                                   text  stars\n",
      "284                                       Great service      5\n",
      "3921                                      Great service      5\n",
      "4372  This review is for the chain in general. The l...      2\n",
      "9680  This review is for the chain in general. The l...      2\n",
      "(9998, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9998 entries, 0 to 9997\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    9998 non-null   object\n",
      " 1   stars   9998 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load yelp review data\n",
    "df = pd.read_csv('./data/yelp.csv')\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.head())\n",
    "# review annotations for 5 classes\n",
    "print(df.stars.value_counts())\n",
    "# always explore the dataset\n",
    "print(len(df.text.unique()))\n",
    "print(df.loc[df.text.duplicated(keep=False), ('text', 'stars')])\n",
    "\n",
    "# dataframe without the duplicated values\n",
    "df = df.loc[~df.text.duplicated(keep='first'), ('text', 'stars')].reset_index(drop=True)\n",
    "print(df.shape)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  stars  \\\n",
      "0  My wife took me here on my birthday for breakf...      5   \n",
      "1  I have no idea why some people give bad review...      5   \n",
      "2  love the gyro plate. Rice is so good and I als...      4   \n",
      "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5   \n",
      "4  General Manager Scott Petello is a good egg!!!...      5   \n",
      "\n",
      "                                          clean_text  \n",
      "0  wife birthday breakfast excellent weather perf...  \n",
      "1  idea people give bad reviews place show gripin...  \n",
      "2      love gyro plate rice good dig candy selection  \n",
      "3  rosie dakota love chaparral dog park s conveni...  \n",
      "4  general manager scott petello good egg detail ...  \n",
      "CPU times: total: 34.6 s\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['clean_text'] = df.text.apply(lambda x: clean_text(x))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['clean_text'] = Parallel(n_jobs=cpu_count()-2, backend='multiprocessing')(delayed(clean_text)(row['text']) for _, row in df.iterrows())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"max_colwidth\", None)\n",
    "# pd.reset_option(\"max_colwidth\")\n",
    "print(df.text.to_list()[0])\n",
    "print()\n",
    "print(df.clean_text.to_list()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(df, df['stars']):\n",
    "    df_train = df.loc[train_index]\n",
    "    df_test = df.loc[test_index]\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_train.stars.value_counts()/len(df_train))\n",
    "print(df_test.stars.value_counts()/len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using bag-of-words\n",
    "count_vect = CountVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "train_count = count_vect.fit_transform(df_train.clean_text.to_numpy())\n",
    "print(len(count_vect.vocabulary_))\n",
    "vocab_dict = {value: key for key, value in count_vect.vocabulary_.items()}\n",
    "print(vocab_dict[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "train_tfidf = tfidf_vect.fit_transform(df_train.clean_text.to_numpy())\n",
    "print(len(tfidf_vect.vocabulary_))\n",
    "vocab_dict = {value: key for key, value in tfidf_vect.vocabulary_.items()}\n",
    "print(vocab_dict[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = df_train.clean_text.to_numpy()\n",
    "train_y = df_train.stars.to_numpy()\n",
    "test_x = df_test.clean_text.to_numpy()\n",
    "test_y = df_test.stars.to_numpy()\n",
    "\n",
    "print(Counter(train_y))\n",
    "print(Counter(test_y))\n",
    "\n",
    "lbl_encoder = LabelEncoder()\n",
    "train_y = lbl_encoder.fit_transform(train_y)\n",
    "test_y = lbl_encoder.transform(test_y)\n",
    "\n",
    "print(Counter(train_y))\n",
    "print(Counter(test_y))\n",
    "\n",
    "# the pipeline with the data transformation and classifer\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", tfidf_vect),\n",
    "        (\"clf\", SVC(class_weight='balanced', random_state=42)),\n",
    "        # (\"clf\", RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# the parameters to be used\n",
    "parameters = {\n",
    "    'vect__max_features': (10000, 15000),\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2)),\n",
    "    \"clf__C\": (1, 10),\n",
    "    \"clf__kernel\": (\"linear\", \"rbf\"),\n",
    "    # 'clf__n_estimators': [100, 200], \n",
    "    # 'clf__max_depth': [3, 5], \n",
    "    # 'clf__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
    "scoring = {\"bal_accuracy\": \"balanced_accuracy\", \"F1_macro\": \"f1_macro\"}\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=cv, scoring=scoring, refit=\"F1_macro\", return_train_score=True)\n",
    "grid_search.fit(df_train.clean_text.to_numpy(), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best score\n",
    "print(grid_search.best_score_)\n",
    "# print the best parameters\n",
    "print(grid_search.best_params_)\n",
    "# the best estimator\n",
    "print(grid_search.best_estimator_)\n",
    "# can also get the best results for each run with different feature combinations\n",
    "cross_val_results = grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = grid_search.best_estimator_.predict(df_test.clean_text.to_numpy())\n",
    "cr_test = classification_report(test_y, test_predictions)\n",
    "cm_test = confusion_matrix(test_y, test_predictions)\n",
    "\n",
    "print(cr_test)\n",
    "print(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf.fit(train_x, train_y)\n",
    "y_pred = dummy_clf.predict(test_x)\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
