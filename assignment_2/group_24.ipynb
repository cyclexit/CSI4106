{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI4106 Assignment 2\n",
    "\n",
    "## Group 24\n",
    "\n",
    "|Name|Student No.|Email\n",
    "|----|-----------|-----\n",
    "|Hongyi Lin| 300053082| hlin087@uottawa.ca\n",
    "|Rodger Retanal| 300052309| rreta014@uottawa.ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3868), started 1 day, 3:33:31 ago. (Use '!kill 3868' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dbbe69fadb197451\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dbbe69fadb197451\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for reproducibility purposes\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# load tensorboard extension\n",
    "%reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the relevant datasets (15/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set.shape = (15026, 4)\n",
      "val_set.shape = (3757, 4)\n",
      "test_set.shape = (4696, 4)\n",
      "columns: Index(['age', 'ethnicity', 'gender', 'img_name'], dtype='object')\n",
      "\n",
      "gender distribution in train_set:\n",
      " 0    2456\n",
      "1    2240\n",
      "Name: gender, dtype: int64\n",
      "gender distribution in val_set:\n",
      " 0    1965\n",
      "1    1792\n",
      "Name: gender, dtype: int64\n",
      "gender distribution in test_set:\n",
      " 0    2456\n",
      "1    2240\n",
      "Name: gender, dtype: int64\n",
      "\n",
      "ethnicity distribution in train_set:\n",
      " 0    1991\n",
      "1     896\n",
      "3     790\n",
      "2     683\n",
      "4     336\n",
      "Name: ethnicity, dtype: int64\n",
      "ethnicity distribution in val_set:\n",
      " 0    1593\n",
      "1     717\n",
      "3     632\n",
      "2     547\n",
      "4     268\n",
      "Name: ethnicity, dtype: int64\n",
      "ethnicity distribution in test_set:\n",
      " 0    1991\n",
      "1     896\n",
      "3     790\n",
      "2     683\n",
      "4     336\n",
      "Name: ethnicity, dtype: int64 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the datasets using the csv files train, val and test \n",
    "# (3)\n",
    "train_set = pd.read_csv(\"./data/train.csv\")\n",
    "val_set = pd.read_csv(\"./data/val.csv\")\n",
    "test_set = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# print the shapes of the dataframes \n",
    "# (3)\n",
    "print(f\"train_set.shape = {train_set.shape}\")\n",
    "print(f\"val_set.shape = {val_set.shape}\")\n",
    "print(f\"test_set.shape = {test_set.shape}\")\n",
    "\n",
    "# print the column names from either one of the dataframes \n",
    "# (1)\n",
    "print(f\"columns: {train_set.columns}\\n\")\n",
    "\n",
    "# print the proportional distribution of gender in all three datasets(i.e., number of male and female) \n",
    "# (3)\n",
    "print(f\"gender distribution in train_set:\\n {test_set['gender'].value_counts()}\")\n",
    "print(f\"gender distribution in val_set:\\n {val_set['gender'].value_counts()}\")\n",
    "print(f\"gender distribution in test_set:\\n {test_set['gender'].value_counts()}\\n\")\n",
    "\n",
    "# print the proportional distribution of ethnicity in all three datasets \n",
    "# (3)\n",
    "print(f\"ethnicity distribution in train_set:\\n {test_set['ethnicity'].value_counts()}\")\n",
    "print(f\"ethnicity distribution in val_set:\\n {val_set['ethnicity'].value_counts()}\")\n",
    "print(f\"ethnicity distribution in test_set:\\n {test_set['ethnicity'].value_counts()} \")\n",
    "\n",
    "# plot the age distribution from the training dataset where the x-axis plots the age\n",
    "# and the y-axis depicts the count of individuals within each age group. For example, individuals with age=1 are:\n",
    "# (2)\n",
    "train_set[\"age\"].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ImageDataGenerators (22/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "(16, 48, 48, 1)\n",
      "(48, 48, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ImageDataGenerator is an iterator.\n",
    "\n",
    "# specify the batch size hyperparameter. You can experiment with different batch sizes\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# create the ImageDataGenerator with rescaling that will generate batched tensors representing images with real-time data augmentation\n",
    "# use at least two of the augmentation strategies. For example, fill_mode='nearest'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (3)\n",
    "train_img_gen = ImageDataGenerator(\n",
    "    rescale= 1.0 / 255,\n",
    "    fill_mode=\"constant\",\n",
    "    cval=0.0\n",
    ")\n",
    "\n",
    "# set up the x_col and y_col\n",
    "x_col = \"img_name\"\n",
    "y_col = list(train_set.columns)\n",
    "y_col.remove(x_col)\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance to link the image folder and the dataframe.\n",
    "# also include the, batch size, image size and the seed.\n",
    "# make sure to include the following arguments\n",
    "# color_mode='grayscale', class_mode='multi_output'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (5)\n",
    "train_itr = train_img_gen.flow_from_dataframe(\n",
    "    dataframe=train_set,\n",
    "    directory=\"./data/images/train/\",\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(48, 48),\n",
    "    seed=SEED,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"multi_output\"\n",
    ")\n",
    "\n",
    "\n",
    "# similarly, create an ImageDataGenerator for the validation dataset and make sure not to use any of th eaugmentation strategies except rescaling the image\n",
    "# (2)\n",
    "val_img_gen = ImageDataGenerator(\n",
    "    rescale= 1.0 / 255\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance with the same arguments as above\n",
    "# make sure to specify the following arguments:\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "val_itr = val_img_gen.flow_from_dataframe(\n",
    "    dataframe=val_set,\n",
    "    directory=\"./data/images/val/\",\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(48, 48),\n",
    "    seed=SEED,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"multi_output\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the val_img_gen instance to link the test dataframe and the test data folder\n",
    "# In addition, make sure to specify the following arguments\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "test_itr = val_img_gen.flow_from_dataframe(\n",
    "    dataframe=test_set,\n",
    "    directory=\"./data/images/test/\",\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(48, 48),\n",
    "    seed=SEED,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"multi_output\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# enumerate through the validation data generator created above and plot first grayscale image \n",
    "# (2)\n",
    "for i, element in enumerate(val_itr):\n",
    "    print(element[0].shape)\n",
    "    tmp = element[0][0]\n",
    "    print(tmp.shape)\n",
    "    plt.imshow(tmp, cmap=plt.cm.binary)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model (44/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"group_24_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 48, 48, 2)    20          input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 24, 24, 2)    0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 1152)         0           max_pooling2d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "age_dense_1 (Dense)             (None, 128)          147584      flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "ethnicity_dense_1 (Dense)       (None, 128)          147584      flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "gender_dense_1 (Dense)          (None, 1)            1153        flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "age_dense_out (Dense)           (None, 128)          16512       age_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ethnicity_dense_out (Dense)     (None, 128)          16512       ethnicity_dense_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "gender_dense_out (Dense)        (None, 1)            2           gender_dense_1[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 329,367\n",
      "Trainable params: 329,367\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "len(train_itr)=940\n",
      "GPU=[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "is_built_with_cuda=True\n",
      "<class 'tensorflow.python.keras.preprocessing.image.DataFrameIterator'>\n",
      "Epoch 1/20\n",
      "58/58 [==============================] - 3s 37ms/step - loss: 1.3927 - age_dense_out_loss: 24.1533 - ethnicity_dense_out_loss: 2.0123 - gender_dense_out_loss: 0.7248 - age_dense_out_mae: 24.1533 - ethnicity_dense_out_accuracy: 0.3792 - gender_dense_out_accuracy: 0.4460 - val_loss: 1.0506 - val_age_dense_out_loss: 16.1504 - val_ethnicity_dense_out_loss: 1.3766 - val_gender_dense_out_loss: 0.6924 - val_age_dense_out_mae: 16.1504 - val_ethnicity_dense_out_accuracy: 0.4417 - val_gender_dense_out_accuracy: 0.4958\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 2/20\n",
      "58/58 [==============================] - 1s 12ms/step - loss: 1.0250 - age_dense_out_loss: 15.2254 - ethnicity_dense_out_loss: 1.3287 - gender_dense_out_loss: 0.6909 - age_dense_out_mae: 15.2254 - ethnicity_dense_out_accuracy: 0.4672 - gender_dense_out_accuracy: 0.5095 - val_loss: 1.0112 - val_age_dense_out_loss: 15.7202 - val_ethnicity_dense_out_loss: 1.3021 - val_gender_dense_out_loss: 0.6889 - val_age_dense_out_mae: 15.7202 - val_ethnicity_dense_out_accuracy: 0.4750 - val_gender_dense_out_accuracy: 0.5208\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 3/20\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 1.0120 - age_dense_out_loss: 15.1579 - ethnicity_dense_out_loss: 1.3059 - gender_dense_out_loss: 0.6877 - age_dense_out_mae: 15.1579 - ethnicity_dense_out_accuracy: 0.4873 - gender_dense_out_accuracy: 0.5456 - val_loss: 0.9625 - val_age_dense_out_loss: 15.1939 - val_ethnicity_dense_out_loss: 1.2110 - val_gender_dense_out_loss: 0.6837 - val_age_dense_out_mae: 15.1939 - val_ethnicity_dense_out_accuracy: 0.5542 - val_gender_dense_out_accuracy: 0.5292\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 4/20\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.9677 - age_dense_out_loss: 14.8291 - ethnicity_dense_out_loss: 1.2252 - gender_dense_out_loss: 0.6805 - age_dense_out_mae: 14.8291 - ethnicity_dense_out_accuracy: 0.5127 - gender_dense_out_accuracy: 0.5159 - val_loss: 0.9546 - val_age_dense_out_loss: 14.9262 - val_ethnicity_dense_out_loss: 1.2034 - val_gender_dense_out_loss: 0.6759 - val_age_dense_out_mae: 14.9262 - val_ethnicity_dense_out_accuracy: 0.5750 - val_gender_dense_out_accuracy: 0.5708\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 5/20\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.9235 - age_dense_out_loss: 14.8416 - ethnicity_dense_out_loss: 1.1447 - gender_dense_out_loss: 0.6727 - age_dense_out_mae: 14.8416 - ethnicity_dense_out_accuracy: 0.5667 - gender_dense_out_accuracy: 0.5424 - val_loss: 0.9077 - val_age_dense_out_loss: 14.8948 - val_ethnicity_dense_out_loss: 1.1179 - val_gender_dense_out_loss: 0.6677 - val_age_dense_out_mae: 14.8948 - val_ethnicity_dense_out_accuracy: 0.6292 - val_gender_dense_out_accuracy: 0.5375\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 6/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.9139 - age_dense_out_loss: 14.5397 - ethnicity_dense_out_loss: 1.1351 - gender_dense_out_loss: 0.6637 - age_dense_out_mae: 14.5397 - ethnicity_dense_out_accuracy: 0.5826 - gender_dense_out_accuracy: 0.5646 - val_loss: 0.8722 - val_age_dense_out_loss: 14.2488 - val_ethnicity_dense_out_loss: 1.0586 - val_gender_dense_out_loss: 0.6573 - val_age_dense_out_mae: 14.2488 - val_ethnicity_dense_out_accuracy: 0.5833 - val_gender_dense_out_accuracy: 0.5792\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 7/20\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.8532 - age_dense_out_loss: 13.9554 - ethnicity_dense_out_loss: 1.0264 - gender_dense_out_loss: 0.6520 - age_dense_out_mae: 13.9554 - ethnicity_dense_out_accuracy: 0.6345 - gender_dense_out_accuracy: 0.6049 - val_loss: 0.8780 - val_age_dense_out_loss: 14.0017 - val_ethnicity_dense_out_loss: 1.0776 - val_gender_dense_out_loss: 0.6504 - val_age_dense_out_mae: 14.0017 - val_ethnicity_dense_out_accuracy: 0.6167 - val_gender_dense_out_accuracy: 0.5625\n",
      "Epoch 8/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8406 - age_dense_out_loss: 13.8335 - ethnicity_dense_out_loss: 1.0172 - gender_dense_out_loss: 0.6362 - age_dense_out_mae: 13.8335 - ethnicity_dense_out_accuracy: 0.6367 - gender_dense_out_accuracy: 0.6239 - val_loss: 0.8441 - val_age_dense_out_loss: 13.6571 - val_ethnicity_dense_out_loss: 1.0211 - val_gender_dense_out_loss: 0.6398 - val_age_dense_out_mae: 13.6571 - val_ethnicity_dense_out_accuracy: 0.6375 - val_gender_dense_out_accuracy: 0.6250\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 9/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8341 - age_dense_out_loss: 14.0430 - ethnicity_dense_out_loss: 0.9986 - gender_dense_out_loss: 0.6415 - age_dense_out_mae: 14.0430 - ethnicity_dense_out_accuracy: 0.6314 - gender_dense_out_accuracy: 0.6472 - val_loss: 0.8644 - val_age_dense_out_loss: 13.7477 - val_ethnicity_dense_out_loss: 1.0710 - val_gender_dense_out_loss: 0.6303 - val_age_dense_out_mae: 13.7477 - val_ethnicity_dense_out_accuracy: 0.5750 - val_gender_dense_out_accuracy: 0.7167\n",
      "Epoch 10/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8363 - age_dense_out_loss: 12.5240 - ethnicity_dense_out_loss: 1.0135 - gender_dense_out_loss: 0.6341 - age_dense_out_mae: 12.5240 - ethnicity_dense_out_accuracy: 0.6261 - gender_dense_out_accuracy: 0.6716 - val_loss: 0.8142 - val_age_dense_out_loss: 13.2348 - val_ethnicity_dense_out_loss: 0.9774 - val_gender_dense_out_loss: 0.6246 - val_age_dense_out_mae: 13.2348 - val_ethnicity_dense_out_accuracy: 0.6250 - val_gender_dense_out_accuracy: 0.6917\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 11/20\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.7940 - age_dense_out_loss: 13.3329 - ethnicity_dense_out_loss: 0.9403 - gender_dense_out_loss: 0.6210 - age_dense_out_mae: 13.3329 - ethnicity_dense_out_accuracy: 0.6727 - gender_dense_out_accuracy: 0.6970 - val_loss: 0.8122 - val_age_dense_out_loss: 12.8550 - val_ethnicity_dense_out_loss: 0.9830 - val_gender_dense_out_loss: 0.6158 - val_age_dense_out_mae: 12.8550 - val_ethnicity_dense_out_accuracy: 0.6583 - val_gender_dense_out_accuracy: 0.7208\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 12/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8014 - age_dense_out_loss: 13.0659 - ethnicity_dense_out_loss: 0.9612 - gender_dense_out_loss: 0.6156 - age_dense_out_mae: 13.0659 - ethnicity_dense_out_accuracy: 0.6494 - gender_dense_out_accuracy: 0.7055 - val_loss: 0.8071 - val_age_dense_out_loss: 12.5409 - val_ethnicity_dense_out_loss: 0.9812 - val_gender_dense_out_loss: 0.6079 - val_age_dense_out_mae: 12.5409 - val_ethnicity_dense_out_accuracy: 0.6000 - val_gender_dense_out_accuracy: 0.7250\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 13/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7676 - age_dense_out_loss: 13.2897 - ethnicity_dense_out_loss: 0.9017 - gender_dense_out_loss: 0.6070 - age_dense_out_mae: 13.2897 - ethnicity_dense_out_accuracy: 0.6886 - gender_dense_out_accuracy: 0.7193 - val_loss: 0.7867 - val_age_dense_out_loss: 12.2243 - val_ethnicity_dense_out_loss: 0.9496 - val_gender_dense_out_loss: 0.5994 - val_age_dense_out_mae: 12.2243 - val_ethnicity_dense_out_accuracy: 0.6708 - val_gender_dense_out_accuracy: 0.7375\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 14/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7709 - age_dense_out_loss: 13.1019 - ethnicity_dense_out_loss: 0.9121 - gender_dense_out_loss: 0.6034 - age_dense_out_mae: 13.1019 - ethnicity_dense_out_accuracy: 0.6737 - gender_dense_out_accuracy: 0.7468 - val_loss: 0.7989 - val_age_dense_out_loss: 12.2877 - val_ethnicity_dense_out_loss: 0.9817 - val_gender_dense_out_loss: 0.5915 - val_age_dense_out_mae: 12.2877 - val_ethnicity_dense_out_accuracy: 0.6375 - val_gender_dense_out_accuracy: 0.7375\n",
      "Epoch 15/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7703 - age_dense_out_loss: 12.0833 - ethnicity_dense_out_loss: 0.9112 - gender_dense_out_loss: 0.6052 - age_dense_out_mae: 12.0833 - ethnicity_dense_out_accuracy: 0.6591 - gender_dense_out_accuracy: 0.7301 - val_loss: 0.7822 - val_age_dense_out_loss: 11.9805 - val_ethnicity_dense_out_loss: 0.9568 - val_gender_dense_out_loss: 0.5838 - val_age_dense_out_mae: 11.9805 - val_ethnicity_dense_out_accuracy: 0.6250 - val_gender_dense_out_accuracy: 0.7667\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 16/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7554 - age_dense_out_loss: 12.2203 - ethnicity_dense_out_loss: 0.9033 - gender_dense_out_loss: 0.5831 - age_dense_out_mae: 12.2203 - ethnicity_dense_out_accuracy: 0.6780 - gender_dense_out_accuracy: 0.7722 - val_loss: 0.7765 - val_age_dense_out_loss: 11.5944 - val_ethnicity_dense_out_loss: 0.9554 - val_gender_dense_out_loss: 0.5745 - val_age_dense_out_mae: 11.5944 - val_ethnicity_dense_out_accuracy: 0.6375 - val_gender_dense_out_accuracy: 0.7583\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 17/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7518 - age_dense_out_loss: 11.8467 - ethnicity_dense_out_loss: 0.8913 - gender_dense_out_loss: 0.5887 - age_dense_out_mae: 11.8467 - ethnicity_dense_out_accuracy: 0.6892 - gender_dense_out_accuracy: 0.7366 - val_loss: 0.7633 - val_age_dense_out_loss: 11.3601 - val_ethnicity_dense_out_loss: 0.9356 - val_gender_dense_out_loss: 0.5683 - val_age_dense_out_mae: 11.3601 - val_ethnicity_dense_out_accuracy: 0.7083 - val_gender_dense_out_accuracy: 0.7625\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 18/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7365 - age_dense_out_loss: 12.2528 - ethnicity_dense_out_loss: 0.8654 - gender_dense_out_loss: 0.5832 - age_dense_out_mae: 12.2528 - ethnicity_dense_out_accuracy: 0.6960 - gender_dense_out_accuracy: 0.7479 - val_loss: 0.7795 - val_age_dense_out_loss: 11.2292 - val_ethnicity_dense_out_loss: 0.9733 - val_gender_dense_out_loss: 0.5631 - val_age_dense_out_mae: 11.2292 - val_ethnicity_dense_out_accuracy: 0.6417 - val_gender_dense_out_accuracy: 0.7792\n",
      "Epoch 19/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7211 - age_dense_out_loss: 11.5784 - ethnicity_dense_out_loss: 0.8433 - gender_dense_out_loss: 0.5758 - age_dense_out_mae: 11.5784 - ethnicity_dense_out_accuracy: 0.7066 - gender_dense_out_accuracy: 0.7733 - val_loss: 0.7440 - val_age_dense_out_loss: 11.4723 - val_ethnicity_dense_out_loss: 0.9096 - val_gender_dense_out_loss: 0.5555 - val_age_dense_out_mae: 11.4723 - val_ethnicity_dense_out_accuracy: 0.6833 - val_gender_dense_out_accuracy: 0.7833\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220331_225120\\assets\n",
      "Epoch 20/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7239 - age_dense_out_loss: 11.5676 - ethnicity_dense_out_loss: 0.8545 - gender_dense_out_loss: 0.5702 - age_dense_out_mae: 11.5676 - ethnicity_dense_out_accuracy: 0.6939 - gender_dense_out_accuracy: 0.7850 - val_loss: 0.7624 - val_age_dense_out_loss: 10.8931 - val_ethnicity_dense_out_loss: 0.9532 - val_gender_dense_out_loss: 0.5498 - val_age_dense_out_mae: 10.8931 - val_ethnicity_dense_out_accuracy: 0.7042 - val_gender_dense_out_accuracy: 0.7750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c025472bb0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the model input with the required shape \n",
    "# (1)\n",
    "inputs = layers.Input(shape=(48, 48, 1))\n",
    "\n",
    "# The shared layers\n",
    "# Include at least one Conv2D layer, MaxPooling2D layer and a Flatten layer\n",
    "# you can have as many layers as possible, but make sure not to overfit your model using the training data\n",
    "# (10)\n",
    "shared_layers = layers.Conv2D(\n",
    "    filters=2,\n",
    "    kernel_size=3,\n",
    "    activation=\"relu\",\n",
    "    strides=(1, 1),\n",
    "    dilation_rate=(1, 1),\n",
    "    padding=\"same\"\n",
    ")(inputs)\n",
    "shared_layers = layers.MaxPooling2D(\n",
    "    pool_size=(2, 2)\n",
    ")(shared_layers)\n",
    "shared_layers = layers.Flatten()(shared_layers)\n",
    "\n",
    "# Task specific layers\n",
    "# Include at least one Dense layer as a task specific layer before generating the output for age\n",
    "# (2)\n",
    "task_layers_age = layers.Dense(units=128, activation=\"relu\", name=\"age_dense_1\")(shared_layers)\n",
    "\n",
    "# Include the age output and make sure to include the following arguments\n",
    "# activation='linear', name='xxx'(any name)\n",
    "# make sure to name your output layers so that different metrics to be used can be linked accordingly\n",
    "# please note that the age prediction is a regression task\n",
    "# (2)\n",
    "AGE_OUT = \"age_dense_out\"\n",
    "outputs_age = layers.Dense(units=128, activation=\"linear\", name=AGE_OUT)(task_layers_age)\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for ethnicity prediction\n",
    "# (2)\n",
    "task_layers_ethnicity = layers.Dense(units=128, activation=\"relu\", name=\"ethnicity_dense_1\")(shared_layers)\n",
    "\n",
    "# Include the ethnicity output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a multi-class classification task\n",
    "# (2)\n",
    "ETHNICITY_OUT = \"ethnicity_dense_out\"\n",
    "outputs_ethnicity = layers.Dense(units=128, activation=\"softmax\", name=ETHNICITY_OUT)(task_layers_ethnicity)\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for gender prediction\n",
    "# (2)\n",
    "task_layers_gender = layers.Dense(units=1, activation=\"sigmoid\", name=\"gender_dense_1\")(shared_layers)\n",
    "\n",
    "# Include the gender output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a binary classification task\n",
    "# (2)\n",
    "GENDER_OUT = \"gender_dense_out\"\n",
    "outputs_gender = layers.Dense(units=1, activation=\"sigmoid\", name=GENDER_OUT)(task_layers_gender)\n",
    "\n",
    "# create the model with the required input and the outputs.\n",
    "# please make sure that the outputs can be included in a list and make sure to keep note of the order\n",
    "# (3)\n",
    "outputs_list = [outputs_age, outputs_ethnicity, outputs_gender]\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs_list, name=\"group_24_model\")\n",
    "\n",
    "# print the model summary\n",
    "# (0.5)\n",
    "print(model.summary())\n",
    "\n",
    "# Instantiate the optimizer with the learning rate. You can start with the learning rate 1e-3(0.001).\n",
    "# Both the optimizer and the learning rate are hyperparameters that you can finetune\n",
    "# For example, you can start with the \"RMSprop\" optimizer\n",
    "# (2)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "\n",
    "# specify the losses to be used for each task: age, ethnicity and gender prediction \n",
    "# (0.5)\n",
    "losses = {\n",
    "    AGE_OUT: \"mae\",\n",
    "    ETHNICITY_OUT: \"sparse_categorical_crossentropy\",\n",
    "    GENDER_OUT: \"binary_crossentropy\"\n",
    "}\n",
    "\n",
    "# compile the model with the optimizer, loss, loss_weights and the metrics for each task\n",
    "# apply the following weights to the losses to balance the contribution of each loss to the total loss\n",
    "# loss_weights=[0.001, 0.5, 0.5]\n",
    "# please remember to use the relevant metric for each task by assigning it to the correct output\n",
    "# (2)\n",
    "loss_weights = {\n",
    "    AGE_OUT: 0.001,\n",
    "    ETHNICITY_OUT: 0.5,\n",
    "    GENDER_OUT: 0.5\n",
    "}\n",
    "metrics = {\n",
    "    AGE_OUT: \"mae\",\n",
    "    ETHNICITY_OUT: \"accuracy\",\n",
    "    GENDER_OUT: \"accuracy\"\n",
    "}\n",
    "model.compile(optimizer=optimizer, loss=losses, loss_weights=loss_weights, metrics=metrics)\n",
    "\n",
    "# Define the callbacks\n",
    "# EarlyStopping: monitor the validation loss while waiting for 3 epochs before stopping\n",
    "# can restore the best weights\n",
    "# (2)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# create dirs for the checkpoint and tensorboard\n",
    "def make_dirs():\n",
    "    d = datetime.datetime.today()\n",
    "    timestamp = d.strftime('%Y%m%d_%H%M%S')\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(os.path.curdir, 'logs', timestamp)\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(os.path.curdir, 'models', timestamp)\n",
    "\n",
    "    os.mkdir(tensorlog_folder)\n",
    "    os.mkdir(checkpoint_folder)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder\n",
    "\n",
    "check_dir, tboard_dir = make_dirs()\n",
    "\n",
    "# ModelCheckpoint\n",
    "# monitor validation loss and save the best model weights\n",
    "# (2)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "# Initiallize TensorBoard\n",
    "# (2)\n",
    "tboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir = tboard_dir\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "# reduce the learning rate by a factor of 0.1 after waiting for 2 epochs while monitoring validation loss\n",
    "# specify a minimum learning rate to be used\n",
    "# (2)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=5e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training and validation generators\n",
    "# In addition please specify the following arguments\n",
    "# steps_per_epoch=len(df_train)/batch_size\n",
    "# validation_steps=len(df_val)/batch_size\n",
    "# (5)\n",
    "print(f\"len(train_itr)={len(train_itr)}\")\n",
    "print(f\"GPU={tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"is_built_with_cuda={tf.test.is_built_with_cuda()}\")\n",
    "print(type(train_itr))\n",
    "# TODO: fix this\n",
    "model.fit(\n",
    "    train_itr,\n",
    "    validation_data=val_itr,\n",
    "    steps_per_epoch=len(train_itr) / BATCH_SIZE,\n",
    "    validation_steps=len(val_itr) / BATCH_SIZE,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    shuffle=True,\n",
    "    callbacks=[reduce_lr, early_stop, checkpoint, tboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions on test data (14/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the trained model using the test generator\n",
    "# print only the test accuracy for ethnicity and gender predictions\n",
    "# (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions using the test generator\n",
    "# (2)\n",
    "\n",
    "# extract the ethnicity predictions\n",
    "# (2)\n",
    "# print the classification report for predicting ethnicity\n",
    "# (2)\n",
    "\n",
    "# extract the gender predictions where probabilities above 0.5 are considered class 1 and if not, class 0\n",
    "# (2)\n",
    "# print the classification report for predicting gender\n",
    "# (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present prediction results on test data(5/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present your findings for 5 different runs by fine-tuning the hyperparameters. The results table must contain the following fields\n",
    "- A minimum of 5 hyperparameters that you have fine-tuned\n",
    "- Mean absolute error for age\n",
    "- Accuracy for ethnicity prediction\n",
    "- Accuracy for gender prediction\n",
    "Please use a table format similar to the one mentioned below when presenting the results.\n",
    "\n",
    "| Hyperparameters | Age(MAE) | Ethnicity(Accuracy)| Gender(Accuracy) |\n",
    "|-----------------|----------|--------------------|------------------|\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n",
    "|                 |          |                    |                  |\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
