{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI4106 Assignment 2\n",
    "\n",
    "## Group 24\n",
    "\n",
    "|Name|Student No.|Email\n",
    "|----|-----------|-----\n",
    "|Hongyi Lin| 300053082| hlin087@uottawa.ca\n",
    "|Rodger Retanal| 300052309| rreta014@uottawa.ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for reproducibility purposes\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "# load tensorboard extension\n",
    "# %reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the relevant datasets (15/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set.shape = (15026, 4)\n",
      "val_set.shape = (3757, 4)\n",
      "test_set.shape = (4696, 4)\n",
      "columns: Index(['age', 'ethnicity', 'gender', 'img_name'], dtype='object')\n",
      "\n",
      "gender distribution in train_set:\n",
      " 0    2456\n",
      "1    2240\n",
      "Name: gender, dtype: int64\n",
      "gender distribution in val_set:\n",
      " 0    1965\n",
      "1    1792\n",
      "Name: gender, dtype: int64\n",
      "gender distribution in test_set:\n",
      " 0    2456\n",
      "1    2240\n",
      "Name: gender, dtype: int64\n",
      "\n",
      "ethnicity distribution in train_set:\n",
      " 0    1991\n",
      "1     896\n",
      "3     790\n",
      "2     683\n",
      "4     336\n",
      "Name: ethnicity, dtype: int64\n",
      "ethnicity distribution in val_set:\n",
      " 0    1593\n",
      "1     717\n",
      "3     632\n",
      "2     547\n",
      "4     268\n",
      "Name: ethnicity, dtype: int64\n",
      "ethnicity distribution in test_set:\n",
      " 0    1991\n",
      "1     896\n",
      "3     790\n",
      "2     683\n",
      "4     336\n",
      "Name: ethnicity, dtype: int64 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the datasets using the csv files train, val and test \n",
    "# (3)\n",
    "train_set = pd.read_csv(\"./data/train.csv\")\n",
    "val_set = pd.read_csv(\"./data/val.csv\")\n",
    "test_set = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# print the shapes of the dataframes \n",
    "# (3)\n",
    "print(f\"train_set.shape = {train_set.shape}\")\n",
    "print(f\"val_set.shape = {val_set.shape}\")\n",
    "print(f\"test_set.shape = {test_set.shape}\")\n",
    "\n",
    "# print the column names from either one of the dataframes \n",
    "# (1)\n",
    "print(f\"columns: {train_set.columns}\\n\")\n",
    "\n",
    "# print the proportional distribution of gender in all three datasets(i.e., number of male and female) \n",
    "# (3)\n",
    "print(f\"gender distribution in train_set:\\n {test_set['gender'].value_counts()}\")\n",
    "print(f\"gender distribution in val_set:\\n {val_set['gender'].value_counts()}\")\n",
    "print(f\"gender distribution in test_set:\\n {test_set['gender'].value_counts()}\\n\")\n",
    "\n",
    "# print the proportional distribution of ethnicity in all three datasets \n",
    "# (3)\n",
    "print(f\"ethnicity distribution in train_set:\\n {test_set['ethnicity'].value_counts()}\")\n",
    "print(f\"ethnicity distribution in val_set:\\n {val_set['ethnicity'].value_counts()}\")\n",
    "print(f\"ethnicity distribution in test_set:\\n {test_set['ethnicity'].value_counts()} \")\n",
    "\n",
    "# plot the age distribution from the training dataset where the x-axis plots the age\n",
    "# and the y-axis depicts the count of individuals within each age group. For example, individuals with age=1 are:\n",
    "# (2)\n",
    "train_set[\"age\"].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ImageDataGenerators (22/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "(16, 48, 48, 1)\n",
      "(48, 48, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ImageDataGenerator is an iterator.\n",
    "\n",
    "# specify the batch size hyperparameter. You can experiment with different batch sizes\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# create the ImageDataGenerator with rescaling that will generate batched tensors representing images with real-time data augmentation\n",
    "# use at least two of the augmentation strategies. For example, fill_mode='nearest'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (3)\n",
    "train_img_gen = ImageDataGenerator(\n",
    "    rescale= 1.0 / 255,\n",
    "    fill_mode=\"constant\",\n",
    "    cval=0.0\n",
    ")\n",
    "\n",
    "# set up the x_col and y_col\n",
    "x_col = \"img_name\"\n",
    "y_col = list(train_set.columns)\n",
    "y_col.remove(x_col)\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance to link the image folder and the dataframe.\n",
    "# also include the, batch size, image size and the seed.\n",
    "# make sure to include the following arguments\n",
    "# color_mode='grayscale', class_mode='multi_output'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (5)\n",
    "train_itr = train_img_gen.flow_from_dataframe(\n",
    "    dataframe=train_set,\n",
    "    directory=\"./data/images/train/\",\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(48, 48),\n",
    "    seed=SEED,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"multi_output\"\n",
    ")\n",
    "\n",
    "\n",
    "# similarly, create an ImageDataGenerator for the validation dataset and make sure not to use any of th eaugmentation strategies except rescaling the image\n",
    "# (2)\n",
    "val_img_gen = ImageDataGenerator(\n",
    "    rescale= 1.0 / 255\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance with the same arguments as above\n",
    "# make sure to specify the following arguments:\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "val_itr = val_img_gen.flow_from_dataframe(\n",
    "    dataframe=val_set,\n",
    "    directory=\"./data/images/val/\",\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(48, 48),\n",
    "    seed=SEED,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"multi_output\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the val_img_gen instance to link the test dataframe and the test data folder\n",
    "# In addition, make sure to specify the following arguments\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "test_itr = val_img_gen.flow_from_dataframe(\n",
    "    dataframe=test_set,\n",
    "    directory=\"./data/images/test/\",\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(48, 48),\n",
    "    seed=SEED,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"multi_output\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# enumerate through the validation data generator created above and plot first grayscale image \n",
    "# (2)\n",
    "for i, element in enumerate(val_itr):\n",
    "    print(element[0].shape)\n",
    "    tmp = element[0][0]\n",
    "    print(tmp.shape)\n",
    "    plt.imshow(tmp, cmap=plt.cm.binary)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model (44/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"group_24_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 48, 48, 8)    80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 24, 24, 8)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4608)         0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "age_dense_1 (Dense)             (None, 128)          589952      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ethnicity_dense_1 (Dense)       (None, 128)          589952      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gender_dense_1 (Dense)          (None, 128)          589952      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "age_dense_out (Dense)           (None, 1)            129         age_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ethnicity_dense_out (Dense)     (None, 5)            645         ethnicity_dense_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "gender_dense_out (Dense)        (None, 1)            129         gender_dense_1[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,770,839\n",
      "Trainable params: 1,770,839\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "len(train_itr)=940\n",
      "GPU=[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "is_built_with_cuda=True\n",
      "<class 'tensorflow.python.keras.preprocessing.image.DataFrameIterator'>\n",
      "Epoch 1/20\n",
      "58/58 [==============================] - 5s 18ms/step - loss: 1.1816 - age_dense_out_loss: 17.5807 - ethnicity_dense_out_loss: 1.5836 - gender_dense_out_loss: 0.7445 - age_dense_out_mae: 17.5807 - ethnicity_dense_out_accuracy: 0.3824 - gender_dense_out_accuracy: 0.6028 - val_loss: 0.9768 - val_age_dense_out_loss: 17.9178 - val_ethnicity_dense_out_loss: 1.3342 - val_gender_dense_out_loss: 0.5837 - val_age_dense_out_mae: 17.9178 - val_ethnicity_dense_out_accuracy: 0.4792 - val_gender_dense_out_accuracy: 0.6750\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 2/20\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 1.0026 - age_dense_out_loss: 16.3816 - ethnicity_dense_out_loss: 1.3624 - gender_dense_out_loss: 0.6100 - age_dense_out_mae: 16.3816 - ethnicity_dense_out_accuracy: 0.4587 - gender_dense_out_accuracy: 0.6663 - val_loss: 0.9119 - val_age_dense_out_loss: 17.5219 - val_ethnicity_dense_out_loss: 1.2683 - val_gender_dense_out_loss: 0.5206 - val_age_dense_out_mae: 17.5219 - val_ethnicity_dense_out_accuracy: 0.5208 - val_gender_dense_out_accuracy: 0.7333\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 3/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.9028 - age_dense_out_loss: 15.8505 - ethnicity_dense_out_loss: 1.2524 - gender_dense_out_loss: 0.5216 - age_dense_out_mae: 15.8505 - ethnicity_dense_out_accuracy: 0.5064 - gender_dense_out_accuracy: 0.7542 - val_loss: 0.8438 - val_age_dense_out_loss: 16.1435 - val_ethnicity_dense_out_loss: 1.1490 - val_gender_dense_out_loss: 0.5064 - val_age_dense_out_mae: 16.1435 - val_ethnicity_dense_out_accuracy: 0.6375 - val_gender_dense_out_accuracy: 0.7208\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 4/20\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.8457 - age_dense_out_loss: 15.0886 - ethnicity_dense_out_loss: 1.1704 - gender_dense_out_loss: 0.4909 - age_dense_out_mae: 15.0886 - ethnicity_dense_out_accuracy: 0.5689 - gender_dense_out_accuracy: 0.7511 - val_loss: 0.7781 - val_age_dense_out_loss: 15.0137 - val_ethnicity_dense_out_loss: 1.0656 - val_gender_dense_out_loss: 0.4606 - val_age_dense_out_mae: 15.0137 - val_ethnicity_dense_out_accuracy: 0.5875 - val_gender_dense_out_accuracy: 0.7500\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 5/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7498 - age_dense_out_loss: 13.8380 - ethnicity_dense_out_loss: 1.0568 - gender_dense_out_loss: 0.4151 - age_dense_out_mae: 13.8380 - ethnicity_dense_out_accuracy: 0.5996 - gender_dense_out_accuracy: 0.8242 - val_loss: 0.7450 - val_age_dense_out_loss: 14.7450 - val_ethnicity_dense_out_loss: 1.0283 - val_gender_dense_out_loss: 0.4322 - val_age_dense_out_mae: 14.7450 - val_ethnicity_dense_out_accuracy: 0.6042 - val_gender_dense_out_accuracy: 0.7833\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 6/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7267 - age_dense_out_loss: 13.1240 - ethnicity_dense_out_loss: 1.0304 - gender_dense_out_loss: 0.3967 - age_dense_out_mae: 13.1240 - ethnicity_dense_out_accuracy: 0.6239 - gender_dense_out_accuracy: 0.8242 - val_loss: 0.6673 - val_age_dense_out_loss: 13.1380 - val_ethnicity_dense_out_loss: 0.9721 - val_gender_dense_out_loss: 0.3363 - val_age_dense_out_mae: 13.1380 - val_ethnicity_dense_out_accuracy: 0.6250 - val_gender_dense_out_accuracy: 0.8625\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 7/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7203 - age_dense_out_loss: 13.3630 - ethnicity_dense_out_loss: 1.0024 - gender_dense_out_loss: 0.4115 - age_dense_out_mae: 13.3630 - ethnicity_dense_out_accuracy: 0.6112 - gender_dense_out_accuracy: 0.8157 - val_loss: 0.6992 - val_age_dense_out_loss: 12.7794 - val_ethnicity_dense_out_loss: 1.0380 - val_gender_dense_out_loss: 0.3349 - val_age_dense_out_mae: 12.7794 - val_ethnicity_dense_out_accuracy: 0.5958 - val_gender_dense_out_accuracy: 0.8583\n",
      "Epoch 8/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6954 - age_dense_out_loss: 12.8802 - ethnicity_dense_out_loss: 0.9516 - gender_dense_out_loss: 0.4135 - age_dense_out_mae: 12.8802 - ethnicity_dense_out_accuracy: 0.6557 - gender_dense_out_accuracy: 0.8008 - val_loss: 0.6586 - val_age_dense_out_loss: 12.5288 - val_ethnicity_dense_out_loss: 0.9494 - val_gender_dense_out_loss: 0.3427 - val_age_dense_out_mae: 12.5288 - val_ethnicity_dense_out_accuracy: 0.6542 - val_gender_dense_out_accuracy: 0.8583\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 9/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6758 - age_dense_out_loss: 12.8953 - ethnicity_dense_out_loss: 0.9293 - gender_dense_out_loss: 0.3965 - age_dense_out_mae: 12.8953 - ethnicity_dense_out_accuracy: 0.6695 - gender_dense_out_accuracy: 0.8157 - val_loss: 0.6665 - val_age_dense_out_loss: 12.0158 - val_ethnicity_dense_out_loss: 0.9386 - val_gender_dense_out_loss: 0.3704 - val_age_dense_out_mae: 12.0158 - val_ethnicity_dense_out_accuracy: 0.6500 - val_gender_dense_out_accuracy: 0.8250\n",
      "Epoch 10/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6228 - age_dense_out_loss: 12.2511 - ethnicity_dense_out_loss: 0.8638 - gender_dense_out_loss: 0.3573 - age_dense_out_mae: 12.2511 - ethnicity_dense_out_accuracy: 0.6960 - gender_dense_out_accuracy: 0.8496 - val_loss: 0.6489 - val_age_dense_out_loss: 11.5379 - val_ethnicity_dense_out_loss: 0.8909 - val_gender_dense_out_loss: 0.3838 - val_age_dense_out_mae: 11.5379 - val_ethnicity_dense_out_accuracy: 0.7000 - val_gender_dense_out_accuracy: 0.8208\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 11/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6038 - age_dense_out_loss: 11.3784 - ethnicity_dense_out_loss: 0.8676 - gender_dense_out_loss: 0.3172 - age_dense_out_mae: 11.3784 - ethnicity_dense_out_accuracy: 0.6780 - gender_dense_out_accuracy: 0.8665 - val_loss: 0.6669 - val_age_dense_out_loss: 11.1355 - val_ethnicity_dense_out_loss: 0.9898 - val_gender_dense_out_loss: 0.3217 - val_age_dense_out_mae: 11.1355 - val_ethnicity_dense_out_accuracy: 0.6625 - val_gender_dense_out_accuracy: 0.8542\n",
      "Epoch 12/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5991 - age_dense_out_loss: 12.0646 - ethnicity_dense_out_loss: 0.8122 - gender_dense_out_loss: 0.3619 - age_dense_out_mae: 12.0646 - ethnicity_dense_out_accuracy: 0.7352 - gender_dense_out_accuracy: 0.8337 - val_loss: 0.6050 - val_age_dense_out_loss: 10.7221 - val_ethnicity_dense_out_loss: 0.8552 - val_gender_dense_out_loss: 0.3333 - val_age_dense_out_mae: 10.7221 - val_ethnicity_dense_out_accuracy: 0.7250 - val_gender_dense_out_accuracy: 0.8458\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 13/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6069 - age_dense_out_loss: 11.4471 - ethnicity_dense_out_loss: 0.8265 - gender_dense_out_loss: 0.3643 - age_dense_out_mae: 11.4471 - ethnicity_dense_out_accuracy: 0.7034 - gender_dense_out_accuracy: 0.8400 - val_loss: 0.5757 - val_age_dense_out_loss: 10.4531 - val_ethnicity_dense_out_loss: 0.8257 - val_gender_dense_out_loss: 0.3047 - val_age_dense_out_mae: 10.4531 - val_ethnicity_dense_out_accuracy: 0.7208 - val_gender_dense_out_accuracy: 0.8542\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 14/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5863 - age_dense_out_loss: 11.8276 - ethnicity_dense_out_loss: 0.8297 - gender_dense_out_loss: 0.3192 - age_dense_out_mae: 11.8276 - ethnicity_dense_out_accuracy: 0.7076 - gender_dense_out_accuracy: 0.8570 - val_loss: 0.6117 - val_age_dense_out_loss: 10.1868 - val_ethnicity_dense_out_loss: 0.8885 - val_gender_dense_out_loss: 0.3146 - val_age_dense_out_mae: 10.1868 - val_ethnicity_dense_out_accuracy: 0.7042 - val_gender_dense_out_accuracy: 0.8542\n",
      "Epoch 15/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5882 - age_dense_out_loss: 11.3261 - ethnicity_dense_out_loss: 0.8076 - gender_dense_out_loss: 0.3461 - age_dense_out_mae: 11.3261 - ethnicity_dense_out_accuracy: 0.7076 - gender_dense_out_accuracy: 0.8347 - val_loss: 0.6237 - val_age_dense_out_loss: 10.0864 - val_ethnicity_dense_out_loss: 0.8809 - val_gender_dense_out_loss: 0.3463 - val_age_dense_out_mae: 10.0864 - val_ethnicity_dense_out_accuracy: 0.7250 - val_gender_dense_out_accuracy: 0.8333\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 16/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5434 - age_dense_out_loss: 10.4290 - ethnicity_dense_out_loss: 0.7628 - gender_dense_out_loss: 0.3031 - age_dense_out_mae: 10.4290 - ethnicity_dense_out_accuracy: 0.7352 - gender_dense_out_accuracy: 0.8485 - val_loss: 0.5717 - val_age_dense_out_loss: 10.1631 - val_ethnicity_dense_out_loss: 0.8202 - val_gender_dense_out_loss: 0.3028 - val_age_dense_out_mae: 10.1631 - val_ethnicity_dense_out_accuracy: 0.7250 - val_gender_dense_out_accuracy: 0.8500\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 17/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5433 - age_dense_out_loss: 10.5921 - ethnicity_dense_out_loss: 0.7510 - gender_dense_out_loss: 0.3144 - age_dense_out_mae: 10.5921 - ethnicity_dense_out_accuracy: 0.7341 - gender_dense_out_accuracy: 0.8633 - val_loss: 0.5788 - val_age_dense_out_loss: 9.9678 - val_ethnicity_dense_out_loss: 0.8217 - val_gender_dense_out_loss: 0.3160 - val_age_dense_out_mae: 9.9678 - val_ethnicity_dense_out_accuracy: 0.7208 - val_gender_dense_out_accuracy: 0.8625\n",
      "Epoch 18/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5563 - age_dense_out_loss: 10.5956 - ethnicity_dense_out_loss: 0.7906 - gender_dense_out_loss: 0.3008 - age_dense_out_mae: 10.5956 - ethnicity_dense_out_accuracy: 0.7288 - gender_dense_out_accuracy: 0.8655 - val_loss: 0.5621 - val_age_dense_out_loss: 9.8906 - val_ethnicity_dense_out_loss: 0.8106 - val_gender_dense_out_loss: 0.2938 - val_age_dense_out_mae: 9.8906 - val_ethnicity_dense_out_accuracy: 0.7250 - val_gender_dense_out_accuracy: 0.8708\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 19/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5073 - age_dense_out_loss: 10.9244 - ethnicity_dense_out_loss: 0.7066 - gender_dense_out_loss: 0.2862 - age_dense_out_mae: 10.9244 - ethnicity_dense_out_accuracy: 0.7669 - gender_dense_out_accuracy: 0.8761 - val_loss: 0.5611 - val_age_dense_out_loss: 9.9112 - val_ethnicity_dense_out_loss: 0.8113 - val_gender_dense_out_loss: 0.2912 - val_age_dense_out_mae: 9.9112 - val_ethnicity_dense_out_accuracy: 0.7292 - val_gender_dense_out_accuracy: 0.8625\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_210540\\assets\n",
      "Epoch 20/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5685 - age_dense_out_loss: 11.4405 - ethnicity_dense_out_loss: 0.8047 - gender_dense_out_loss: 0.3095 - age_dense_out_mae: 11.4405 - ethnicity_dense_out_accuracy: 0.7161 - gender_dense_out_accuracy: 0.8517 - val_loss: 0.5629 - val_age_dense_out_loss: 9.8916 - val_ethnicity_dense_out_loss: 0.8054 - val_gender_dense_out_loss: 0.3005 - val_age_dense_out_mae: 9.8916 - val_ethnicity_dense_out_accuracy: 0.7208 - val_gender_dense_out_accuracy: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f00610b430>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the model input with the required shape \n",
    "# (1)\n",
    "inputs = layers.Input(shape=(48, 48, 1))\n",
    "\n",
    "# The shared layers\n",
    "# Include at least one Conv2D layer, MaxPooling2D layer and a Flatten layer\n",
    "# you can have as many layers as possible, but make sure not to overfit your model using the training data\n",
    "# (10)\n",
    "shared_layers = layers.Conv2D(\n",
    "    filters=8,\n",
    "    kernel_size=3,\n",
    "    activation=\"relu\",\n",
    "    strides=(1, 1),\n",
    "    dilation_rate=(1, 1),\n",
    "    padding=\"same\"\n",
    ")(inputs)\n",
    "shared_layers = layers.MaxPooling2D(\n",
    "    pool_size=(2, 2)\n",
    ")(shared_layers)\n",
    "shared_layers = layers.Flatten()(shared_layers)\n",
    "\n",
    "# Task specific layers\n",
    "# Include at least one Dense layer as a task specific layer before generating the output for age\n",
    "# (2)\n",
    "task_layers_age = layers.Dense(units=128, activation=\"relu\", name=\"age_dense_1\")(shared_layers)\n",
    "\n",
    "# Include the age output and make sure to include the following arguments\n",
    "# activation='linear', name='xxx'(any name)\n",
    "# make sure to name your output layers so that different metrics to be used can be linked accordingly\n",
    "# please note that the age prediction is a regression task\n",
    "# (2)\n",
    "AGE_OUT = \"age_dense_out\"\n",
    "outputs_age = layers.Dense(units=1, activation=\"linear\", name=AGE_OUT)(task_layers_age) # NOTE: regression units=1\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for ethnicity prediction\n",
    "# (2)\n",
    "task_layers_ethnicity = layers.Dense(units=128, activation=\"relu\", name=\"ethnicity_dense_1\")(shared_layers)\n",
    "\n",
    "# Include the ethnicity output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a multi-class classification task\n",
    "# (2)\n",
    "ETHNICITY_OUT = \"ethnicity_dense_out\"\n",
    "outputs_ethnicity = layers.Dense(units=5, activation=\"softmax\", name=ETHNICITY_OUT)(task_layers_ethnicity) # NOTE: multi-class classification, units=num of classes\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for gender prediction\n",
    "# (2)\n",
    "task_layers_gender = layers.Dense(units=128, activation=\"relu\", name=\"gender_dense_1\")(shared_layers) # NOTE: task layers unit can be more than 1\n",
    "\n",
    "# Include the gender output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a binary classification task\n",
    "# (2)\n",
    "GENDER_OUT = \"gender_dense_out\"\n",
    "outputs_gender = layers.Dense(units=1, activation=\"sigmoid\", name=GENDER_OUT)(task_layers_gender) # NOTE: sigmoid, units=1 only\n",
    "\n",
    "# create the model with the required input and the outputs.\n",
    "# please make sure that the outputs can be included in a list and make sure to keep note of the order\n",
    "# (3)\n",
    "outputs_list = [outputs_age, outputs_ethnicity, outputs_gender]\n",
    "outputs_idx = {\n",
    "    \"age\": 0,\n",
    "    \"ethnicity\": 1,\n",
    "    \"gender\": 2\n",
    "}\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs_list, name=\"group_24_model\")\n",
    "\n",
    "# print the model summary\n",
    "# (0.5)\n",
    "print(model.summary())\n",
    "\n",
    "# Instantiate the optimizer with the learning rate. You can start with the learning rate 1e-3(0.001).\n",
    "# Both the optimizer and the learning rate are hyperparameters that you can finetune\n",
    "# For example, you can start with the \"RMSprop\" optimizer\n",
    "# (2)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "\n",
    "# specify the losses to be used for each task: age, ethnicity and gender prediction \n",
    "# (0.5)\n",
    "LOSSES = {\n",
    "    AGE_OUT: \"mae\",\n",
    "    ETHNICITY_OUT: \"sparse_categorical_crossentropy\",\n",
    "    GENDER_OUT: \"binary_crossentropy\"\n",
    "}\n",
    "\n",
    "# compile the model with the optimizer, loss, loss_weights and the metrics for each task\n",
    "# apply the following weights to the losses to balance the contribution of each loss to the total loss\n",
    "# loss_weights=[0.001, 0.5, 0.5]\n",
    "# please remember to use the relevant metric for each task by assigning it to the correct output\n",
    "# (2)\n",
    "LOSS_WEIGHTS = {\n",
    "    AGE_OUT: 0.001,\n",
    "    ETHNICITY_OUT: 0.5,\n",
    "    GENDER_OUT: 0.5\n",
    "}\n",
    "METRICS = {\n",
    "    AGE_OUT: \"mae\",\n",
    "    ETHNICITY_OUT: \"accuracy\",\n",
    "    GENDER_OUT: \"accuracy\"\n",
    "}\n",
    "model.compile(optimizer=optimizer, loss=LOSSES, loss_weights=LOSS_WEIGHTS, metrics=METRICS)\n",
    "\n",
    "# Define the callbacks\n",
    "# EarlyStopping: monitor the validation loss while waiting for 3 epochs before stopping\n",
    "# can restore the best weights\n",
    "# (2)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# create dirs for the checkpoint and tensorboard\n",
    "def make_dirs():\n",
    "    d = datetime.datetime.today()\n",
    "    timestamp = d.strftime('%Y%m%d_%H%M%S')\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(os.path.curdir, 'logs', timestamp)\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(os.path.curdir, 'models', timestamp)\n",
    "\n",
    "    Path(tensorlog_folder).mkdir(exist_ok=True, parents=True)\n",
    "    Path(checkpoint_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder\n",
    "\n",
    "check_dir, tboard_dir = make_dirs()\n",
    "\n",
    "# ModelCheckpoint\n",
    "# monitor validation loss and save the best model weights\n",
    "# (2)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "# Initiallize TensorBoard\n",
    "# (2)\n",
    "tboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir = tboard_dir\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "# reduce the learning rate by a factor of 0.1 after waiting for 2 epochs while monitoring validation loss\n",
    "# specify a minimum learning rate to be used\n",
    "# (2)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=5e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training and validation generators\n",
    "# In addition please specify the following arguments\n",
    "# steps_per_epoch=len(df_train)/batch_size\n",
    "# validation_steps=len(df_val)/batch_size\n",
    "# (5)\n",
    "print(f\"len(train_itr)={len(train_itr)}\")\n",
    "print(f\"GPU={tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"is_built_with_cuda={tf.test.is_built_with_cuda()}\")\n",
    "print(type(train_itr))\n",
    "model.fit(\n",
    "    train_itr,\n",
    "    validation_data=val_itr,\n",
    "    steps_per_epoch=len(train_itr) / BATCH_SIZE,\n",
    "    validation_steps=len(val_itr) / BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    shuffle=True,\n",
    "    callbacks=[reduce_lr, early_stop, checkpoint, tboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions on test data (14/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5771 - age_dense_out_loss: 10.8378 - ethnicity_dense_out_loss: 0.8108 - gender_dense_out_loss: 0.3217 - age_dense_out_mae: 10.8378 - ethnicity_dense_out_accuracy: 0.7206 - gender_dense_out_accuracy: 0.8588\n",
      "\n",
      "Test evaluation:\n",
      "[0.5770628452301025, 10.837848663330078, 0.8107830286026001, 0.32166674733161926, 10.837848663330078, 0.7206133008003235, 0.8588160276412964]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trained model using the test generator\n",
    "# print only the test accuracy for ethnicity and gender predictions\n",
    "# (4)\n",
    "test_evals = model.evaluate(test_itr, verbose=1)\n",
    "print(f\"\\nTest evaluation:\\n{test_evals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 1s 4ms/step\n",
      "Ethnicity Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.83      0.78      1991\n",
      "           1       0.75      0.78      0.76       896\n",
      "           2       0.72      0.73      0.72       683\n",
      "           3       0.65      0.66      0.65       790\n",
      "           4       0.43      0.04      0.07       336\n",
      "\n",
      "    accuracy                           0.72      4696\n",
      "   macro avg       0.66      0.61      0.60      4696\n",
      "weighted avg       0.70      0.72      0.70      4696\n",
      "\n",
      "Gender Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86      2456\n",
      "           1       0.84      0.87      0.85      2240\n",
      "\n",
      "    accuracy                           0.86      4696\n",
      "   macro avg       0.86      0.86      0.86      4696\n",
      "weighted avg       0.86      0.86      0.86      4696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate predictions using the test generator\n",
    "# (2)\n",
    "predictions = model.predict(test_itr, verbose=1)\n",
    "# print(f\"\\nPredictions:\\n{predictions}\")\n",
    "\n",
    "# extract the ethnicity predictions\n",
    "# (2)\n",
    "# NOTE: we can only extract by remembering the order? 2nd is ethnicity\n",
    "predictions_ethnicity = np.argmax(predictions[outputs_idx[\"ethnicity\"]], axis=1)\n",
    "# print(predictions_ethnicity)\n",
    "\n",
    "# print the classification report for predicting ethnicity\n",
    "# (2)\n",
    "cr_ethnicity = classification_report(test_set[\"ethnicity\"].values, predictions_ethnicity)\n",
    "print(f\"Ethnicity Classification Report:\\n{cr_ethnicity}\")\n",
    "\n",
    "# extract the gender predictions where probabilities above 0.5 are considered class 1 and if not, class 0\n",
    "# (2)\n",
    "predictions_gender = predictions[outputs_idx[\"gender\"]].flatten().tolist()\n",
    "for i in range(0, len(predictions_gender)):\n",
    "    predictions_gender[i] = 1 if predictions_gender[i] > 0.5 else 0\n",
    "# print(predictions_gender)\n",
    "\n",
    "# print the classification report for predicting gender\n",
    "# (2)\n",
    "cr_gender = classification_report(test_set[\"gender\"].values, predictions_gender)\n",
    "print(f\"Gender Classification Report:\\n{cr_gender}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present prediction results on test data(5/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "HP_BATCH_SIZE = hp.HParam(\"batch_size\", hp.Discrete([8, 16]))\n",
    "HP_OPTIMIZER = hp.HParam(\"optimizer\", hp.Discrete([\"RMSprop\", \"Adam\", \"Adamax\"]))\n",
    "HP_FILTERS = hp.HParam(\"units\", hp.Discrete([8, 16, 32]))\n",
    "HP_UNITS = hp.HParam(\"units\", hp.Discrete([64, 128, 256]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_itr(bactch_size: int):\n",
    "    train_itr = train_img_gen.flow_from_dataframe(\n",
    "        dataframe=train_set,\n",
    "        directory=\"./data/images/train/\",\n",
    "        x_col=x_col,\n",
    "        y_col=y_col,\n",
    "        batch_size=bactch_size,\n",
    "        target_size=(48, 48),\n",
    "        seed=SEED,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=\"multi_output\"\n",
    "    )\n",
    "    val_itr = val_img_gen.flow_from_dataframe(\n",
    "        dataframe=val_set,\n",
    "        directory=\"./data/images/val/\",\n",
    "        x_col=x_col,\n",
    "        y_col=y_col,\n",
    "        batch_size=bactch_size,\n",
    "        target_size=(48, 48),\n",
    "        seed=SEED,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=\"multi_output\",\n",
    "        shuffle=False\n",
    "    )\n",
    "    test_itr = val_img_gen.flow_from_dataframe(\n",
    "        dataframe=test_set,\n",
    "        directory=\"./data/images/test/\",\n",
    "        x_col=x_col,\n",
    "        y_col=y_col,\n",
    "        batch_size=bactch_size,\n",
    "        target_size=(48, 48),\n",
    "        seed=SEED,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=\"multi_output\",\n",
    "        shuffle=False\n",
    "    )\n",
    "    return {\"train\": train_itr, \"val\": val_itr, \"test\": test_itr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(filters, units, id):\n",
    "    # input\n",
    "    inputs = layers.Input(shape=(48, 48, 1))\n",
    "\n",
    "    # shared layers\n",
    "    shared_layers = layers.Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\",\n",
    "        strides=(1, 1),\n",
    "        dilation_rate=(1, 1),\n",
    "        padding=\"same\"\n",
    "    )(inputs)\n",
    "    shared_layers = layers.MaxPooling2D(\n",
    "        pool_size=(2, 2)\n",
    "    )(shared_layers)\n",
    "    shared_layers = layers.Flatten()(shared_layers)\n",
    "\n",
    "    # task specific layers\n",
    "    task_layers_age = layers.Dense(units=units, activation=\"relu\", name=\"age_dense_1\")(shared_layers)\n",
    "    outputs_age = layers.Dense(units=1, activation=\"linear\", name=AGE_OUT)(task_layers_age)\n",
    "\n",
    "    task_layers_ethnicity = layers.Dense(units=units, activation=\"relu\", name=\"ethnicity_dense_1\")(shared_layers)\n",
    "    outputs_ethnicity = layers.Dense(units=5, activation=\"softmax\", name=ETHNICITY_OUT)(task_layers_ethnicity)\n",
    "    \n",
    "    task_layers_gender = layers.Dense(units=units, activation=\"relu\", name=\"gender_dense_1\")(shared_layers)\n",
    "    outputs_gender = layers.Dense(units=1, activation=\"sigmoid\", name=GENDER_OUT)(task_layers_gender)\n",
    "\n",
    "    # create the model\n",
    "    outputs_list = [outputs_age, outputs_ethnicity, outputs_gender]\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs_list, name=\"group_24_model_\" + str(id))\n",
    "    return model\n",
    "\n",
    "def make_session_dirs(parent_dir, session):\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(parent_dir, \"logs\", str(session))\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(parent_dir, \"models\", str(session))\n",
    "\n",
    "    Path(checkpoint_folder).mkdir(parents=True, exist_ok=True)\n",
    "    Path(tensorlog_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder\n",
    "\n",
    "def get_callbacks(dir, session):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    check_dir, tboard_dir = make_session_dirs(dir, session)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=check_dir,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False\n",
    "    )\n",
    "    tboard = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = tboard_dir\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.1,\n",
    "        patience=2,\n",
    "        min_lr=5e-5,\n",
    "        verbose=1\n",
    "    )\n",
    "    return [reduce_lr, early_stop, checkpoint, tboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_hparams(hparams, dir, session):\n",
    "    batch_size = hparams[HP_BATCH_SIZE]\n",
    "    itrs = get_df_itr(batch_size)\n",
    "\n",
    "    model = create_model(\n",
    "        filters=hparams[HP_FILTERS],\n",
    "        units=hparams[HP_UNITS],\n",
    "        id=session\n",
    "    )\n",
    "    model.compile(optimizer=hparams[HP_OPTIMIZER], loss=LOSSES, loss_weights=LOSS_WEIGHTS, metrics=METRICS)\n",
    "\n",
    "    callbacks = get_callbacks(dir, session)\n",
    "    \n",
    "    model.fit(\n",
    "        itrs[\"train\"],\n",
    "        validation_data=itrs[\"val\"],\n",
    "        steps_per_epoch=len(itrs[\"train\"]) / batch_size,\n",
    "        validation_steps=len(itrs[\"val\"]) / batch_size,\n",
    "        batch_size = batch_size,\n",
    "        epochs=20,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    test_evals = model.evaluate(itrs[\"test\"], verbose=1)\n",
    "    print(f\"Test evaluation:\\n{test_evals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Session 0------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\0\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5597 - age_dense_out_loss: 9.7697 - ethnicity_dense_out_loss: 0.7885 - gender_dense_out_loss: 0.3114 - age_dense_out_mae: 9.7697 - ethnicity_dense_out_accuracy: 0.7279 - gender_dense_out_accuracy: 0.8629\n",
      "Test evaluation:\n",
      "[0.5597237944602966, 9.769737243652344, 0.7884833812713623, 0.31142425537109375, 9.769737243652344, 0.7278534770011902, 0.8628619909286499]\n",
      "\n",
      "------Session 1------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\1\\assets\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.5498 - age_dense_out_loss: 9.5611 - ethnicity_dense_out_loss: 0.7680 - gender_dense_out_loss: 0.3124 - age_dense_out_mae: 9.5611 - ethnicity_dense_out_accuracy: 0.7372 - gender_dense_out_accuracy: 0.8663\n",
      "Test evaluation:\n",
      "[0.5497575402259827, 9.561076164245605, 0.7680269479751587, 0.31236550211906433, 9.561076164245605, 0.7372231483459473, 0.8662691712379456]\n",
      "\n",
      "------Session 2------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\2\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5465 - age_dense_out_loss: 9.1489 - ethnicity_dense_out_loss: 0.7628 - gender_dense_out_loss: 0.3119 - age_dense_out_mae: 9.1489 - ethnicity_dense_out_accuracy: 0.7400 - gender_dense_out_accuracy: 0.8590\n",
      "Test evaluation:\n",
      "[0.546481728553772, 9.14892292022705, 0.7627761363983154, 0.3118889033794403, 9.14892292022705, 0.7399914860725403, 0.8590289354324341]\n",
      "\n",
      "------Session 3------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\3\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5487 - age_dense_out_loss: 9.5427 - ethnicity_dense_out_loss: 0.7680 - gender_dense_out_loss: 0.3103 - age_dense_out_mae: 9.5427 - ethnicity_dense_out_accuracy: 0.7385 - gender_dense_out_accuracy: 0.8673\n",
      "Test evaluation:\n",
      "[0.5487003326416016, 9.542675971984863, 0.7679950594902039, 0.3103199601173401, 9.542675971984863, 0.7385008335113525, 0.8673338890075684]\n",
      "\n",
      "------Session 4------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\4\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5623 - age_dense_out_loss: 9.5605 - ethnicity_dense_out_loss: 0.7891 - gender_dense_out_loss: 0.3164 - age_dense_out_mae: 9.5605 - ethnicity_dense_out_accuracy: 0.7276 - gender_dense_out_accuracy: 0.8626\n",
      "Test evaluation:\n",
      "[0.5622953772544861, 9.560510635375977, 0.7890980839729309, 0.31637129187583923, 9.560510635375977, 0.7276405692100525, 0.8626490831375122]\n",
      "\n",
      "------Session 5------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\5\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5507 - age_dense_out_loss: 9.2489 - ethnicity_dense_out_loss: 0.7607 - gender_dense_out_loss: 0.3221 - age_dense_out_mae: 9.2489 - ethnicity_dense_out_accuracy: 0.7440 - gender_dense_out_accuracy: 0.8603\n",
      "Test evaluation:\n",
      "[0.5506810545921326, 9.248855590820312, 0.7607457637786865, 0.3221185803413391, 9.248855590820312, 0.7440374493598938, 0.8603066205978394]\n",
      "\n",
      "------Session 6------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\6\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5683 - age_dense_out_loss: 9.7803 - ethnicity_dense_out_loss: 0.7926 - gender_dense_out_loss: 0.3245 - age_dense_out_mae: 9.7803 - ethnicity_dense_out_accuracy: 0.7272 - gender_dense_out_accuracy: 0.8641\n",
      "Test evaluation:\n",
      "[0.5683124661445618, 9.78026294708252, 0.7925791144371033, 0.32448461651802063, 9.78026294708252, 0.7272146344184875, 0.8641396760940552]\n",
      "\n",
      "------Session 7------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\7\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5215 - age_dense_out_loss: 9.0172 - ethnicity_dense_out_loss: 0.7423 - gender_dense_out_loss: 0.2827 - age_dense_out_mae: 9.0172 - ethnicity_dense_out_accuracy: 0.7481 - gender_dense_out_accuracy: 0.8754\n",
      "Test evaluation:\n",
      "[0.521536648273468, 9.017163276672363, 0.7423223853111267, 0.28271692991256714, 9.017163276672363, 0.7480834722518921, 0.8754258751869202]\n",
      "\n",
      "------Session 8------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\8\\assets\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.5247 - age_dense_out_loss: 8.6618 - ethnicity_dense_out_loss: 0.7255 - gender_dense_out_loss: 0.3067 - age_dense_out_mae: 8.6618 - ethnicity_dense_out_accuracy: 0.7594 - gender_dense_out_accuracy: 0.8644\n",
      "Test evaluation:\n",
      "[0.5247336626052856, 8.661837577819824, 0.7254871726036072, 0.3066561818122864, 8.661837577819824, 0.7593696713447571, 0.8643526434898376]\n",
      "\n",
      "------Session 9------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\9\\assets\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.6355 - age_dense_out_loss: 11.0867 - ethnicity_dense_out_loss: 0.8811 - gender_dense_out_loss: 0.3678 - age_dense_out_mae: 11.0867 - ethnicity_dense_out_accuracy: 0.6825 - gender_dense_out_accuracy: 0.8330\n",
      "Test evaluation:\n",
      "[0.6355382800102234, 11.086722373962402, 0.8811038136482239, 0.36780011653900146, 11.086722373962402, 0.6824957132339478, 0.8330494165420532]\n",
      "\n",
      "------Session 10------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\10\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5815 - age_dense_out_loss: 10.2052 - ethnicity_dense_out_loss: 0.8221 - gender_dense_out_loss: 0.3204 - age_dense_out_mae: 10.2052 - ethnicity_dense_out_accuracy: 0.7178 - gender_dense_out_accuracy: 0.8605\n",
      "Test evaluation:\n",
      "[0.5815025568008423, 10.205191612243652, 0.8221495151519775, 0.32044586539268494, 10.205191612243652, 0.7178449630737305, 0.8605195879936218]\n",
      "\n",
      "------Session 11------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\11\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.6050 - age_dense_out_loss: 10.4965 - ethnicity_dense_out_loss: 0.8486 - gender_dense_out_loss: 0.3404 - age_dense_out_mae: 10.4965 - ethnicity_dense_out_accuracy: 0.7083 - gender_dense_out_accuracy: 0.8480\n",
      "Test evaluation:\n",
      "[0.6049799919128418, 10.496516227722168, 0.8486122488975525, 0.34035465121269226, 10.496516227722168, 0.7082623243331909, 0.8479557037353516]\n",
      "\n",
      "------Session 12------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\12\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5927 - age_dense_out_loss: 10.3023 - ethnicity_dense_out_loss: 0.8257 - gender_dense_out_loss: 0.3390 - age_dense_out_mae: 10.3023 - ethnicity_dense_out_accuracy: 0.7159 - gender_dense_out_accuracy: 0.8514\n",
      "Test evaluation:\n",
      "[0.5926569104194641, 10.302321434020996, 0.8256754279136658, 0.3390332758426666, 10.302321434020996, 0.7159284353256226, 0.8513628840446472]\n",
      "\n",
      "------Session 13------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\13\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5734 - age_dense_out_loss: 10.9184 - ethnicity_dense_out_loss: 0.7959 - gender_dense_out_loss: 0.3290 - age_dense_out_mae: 10.9184 - ethnicity_dense_out_accuracy: 0.7221 - gender_dense_out_accuracy: 0.8626\n",
      "Test evaluation:\n",
      "[0.5733738541603088, 10.918381690979004, 0.7958976626396179, 0.32901304960250854, 10.918381690979004, 0.7221038937568665, 0.8626490831375122]\n",
      "\n",
      "------Session 14------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\14\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5925 - age_dense_out_loss: 10.5777 - ethnicity_dense_out_loss: 0.8096 - gender_dense_out_loss: 0.3542 - age_dense_out_mae: 10.5777 - ethnicity_dense_out_accuracy: 0.7189 - gender_dense_out_accuracy: 0.8428\n",
      "Test evaluation:\n",
      "[0.5924705266952515, 10.577730178833008, 0.8095541596412659, 0.35423117876052856, 10.577730178833008, 0.7189096808433533, 0.8428449630737305]\n",
      "\n",
      "------Session 15------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\15\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5532 - age_dense_out_loss: 10.0131 - ethnicity_dense_out_loss: 0.7762 - gender_dense_out_loss: 0.3102 - age_dense_out_mae: 10.0131 - ethnicity_dense_out_accuracy: 0.7362 - gender_dense_out_accuracy: 0.8673\n",
      "Test evaluation:\n",
      "[0.5531995296478271, 10.013059616088867, 0.7761661410331726, 0.3102063536643982, 10.013059616088867, 0.7361584305763245, 0.8673338890075684]\n",
      "\n",
      "------Session 16------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\16\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5561 - age_dense_out_loss: 9.8076 - ethnicity_dense_out_loss: 0.7711 - gender_dense_out_loss: 0.3215 - age_dense_out_mae: 9.8076 - ethnicity_dense_out_accuracy: 0.7351 - gender_dense_out_accuracy: 0.8620\n",
      "Test evaluation:\n",
      "[0.5561084747314453, 9.807565689086914, 0.7710610032081604, 0.32154032588005066, 9.807565689086914, 0.7350937128067017, 0.8620102405548096]\n",
      "\n",
      "------Session 17------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\17\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5915 - age_dense_out_loss: 10.2906 - ethnicity_dense_out_loss: 0.8329 - gender_dense_out_loss: 0.3296 - age_dense_out_mae: 10.2906 - ethnicity_dense_out_accuracy: 0.7129 - gender_dense_out_accuracy: 0.8614\n",
      "Test evaluation:\n",
      "[0.5915382504463196, 10.290621757507324, 0.8328728079795837, 0.3296232521533966, 10.290621757507324, 0.7129471898078918, 0.8613713979721069]\n",
      "\n",
      "------Session 18------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\18\\assets\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.6192 - age_dense_out_loss: 9.6343 - ethnicity_dense_out_loss: 0.7861 - gender_dense_out_loss: 0.4330 - age_dense_out_mae: 9.6343 - ethnicity_dense_out_accuracy: 0.7317 - gender_dense_out_accuracy: 0.8275\n",
      "Test evaluation:\n",
      "[0.6191778779029846, 9.634269714355469, 0.7860848903656006, 0.4330020248889923, 9.634269714355469, 0.731686532497406, 0.827512800693512]\n",
      "\n",
      "------Session 19------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\19\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5629 - age_dense_out_loss: 9.9608 - ethnicity_dense_out_loss: 0.7834 - gender_dense_out_loss: 0.3225 - age_dense_out_mae: 9.9608 - ethnicity_dense_out_accuracy: 0.7362 - gender_dense_out_accuracy: 0.8569\n",
      "Test evaluation:\n",
      "[0.5629147291183472, 9.960765838623047, 0.7834139466285706, 0.32249364256858826, 9.960765838623047, 0.7361584305763245, 0.8568994998931885]\n",
      "\n",
      "------Session 20------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\20\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5529 - age_dense_out_loss: 9.6676 - ethnicity_dense_out_loss: 0.7685 - gender_dense_out_loss: 0.3179 - age_dense_out_mae: 9.6676 - ethnicity_dense_out_accuracy: 0.7342 - gender_dense_out_accuracy: 0.8620\n",
      "Test evaluation:\n",
      "[0.5528589487075806, 9.66762638092041, 0.7685121893882751, 0.3178713917732239, 9.66762638092041, 0.7342419028282166, 0.8620102405548096]\n",
      "\n",
      "------Session 21------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\21\\assets\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\21\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\21\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5614 - age_dense_out_loss: 10.1560 - ethnicity_dense_out_loss: 0.7735 - gender_dense_out_loss: 0.3290 - age_dense_out_mae: 10.1560 - ethnicity_dense_out_accuracy: 0.7387 - gender_dense_out_accuracy: 0.8582\n",
      "Test evaluation:\n",
      "[0.5613901019096375, 10.156004905700684, 0.7734714150428772, 0.32899710536003113, 10.156004905700684, 0.738713800907135, 0.8581771850585938]\n",
      "\n",
      "------Session 22------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\22\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5379 - age_dense_out_loss: 9.6497 - ethnicity_dense_out_loss: 0.7534 - gender_dense_out_loss: 0.3031 - age_dense_out_mae: 9.6497 - ethnicity_dense_out_accuracy: 0.7428 - gender_dense_out_accuracy: 0.8669\n",
      "Test evaluation:\n",
      "[0.537886381149292, 9.649724006652832, 0.753352165222168, 0.30312031507492065, 9.649724006652832, 0.7427598237991333, 0.8669080138206482]\n",
      "\n",
      "------Session 23------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\23\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5397 - age_dense_out_loss: 8.9024 - ethnicity_dense_out_loss: 0.7535 - gender_dense_out_loss: 0.3080 - age_dense_out_mae: 8.9024 - ethnicity_dense_out_accuracy: 0.7408 - gender_dense_out_accuracy: 0.8661\n",
      "Test evaluation:\n",
      "[0.5396543741226196, 8.902438163757324, 0.7534870505332947, 0.3080177307128906, 8.902438163757324, 0.7408432960510254, 0.8660562038421631]\n",
      "\n",
      "------Session 24------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\24\\assets\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\24\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5470 - age_dense_out_loss: 9.8049 - ethnicity_dense_out_loss: 0.7563 - gender_dense_out_loss: 0.3182 - age_dense_out_mae: 9.8049 - ethnicity_dense_out_accuracy: 0.7413 - gender_dense_out_accuracy: 0.8639\n",
      "Test evaluation:\n",
      "[0.5470388531684875, 9.804933547973633, 0.756304144859314, 0.31816381216049194, 9.804933547973633, 0.7412691712379456, 0.8639267683029175]\n",
      "\n",
      "------Session 25------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\25\\assets\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\25\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\25\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5338 - age_dense_out_loss: 9.2032 - ethnicity_dense_out_loss: 0.7538 - gender_dense_out_loss: 0.2955 - age_dense_out_mae: 9.2032 - ethnicity_dense_out_accuracy: 0.7489 - gender_dense_out_accuracy: 0.8737\n",
      "Test evaluation:\n",
      "[0.5338478088378906, 9.203226089477539, 0.7537721991539001, 0.2955167293548584, 9.203226089477539, 0.7489352822303772, 0.8737223148345947]\n",
      "\n",
      "------Session 26------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\26\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5340 - age_dense_out_loss: 8.9216 - ethnicity_dense_out_loss: 0.7477 - gender_dense_out_loss: 0.3024 - age_dense_out_mae: 8.9216 - ethnicity_dense_out_accuracy: 0.7500 - gender_dense_out_accuracy: 0.8703\n",
      "Test evaluation:\n",
      "[0.5339732766151428, 8.921612739562988, 0.7477033138275146, 0.3024002015590668, 8.921612739562988, 0.75, 0.8703151345252991]\n",
      "\n",
      "------Session 27------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\27\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6869 - age_dense_out_loss: 11.5309 - ethnicity_dense_out_loss: 0.9584 - gender_dense_out_loss: 0.3924 - age_dense_out_mae: 11.5309 - ethnicity_dense_out_accuracy: 0.6618 - gender_dense_out_accuracy: 0.8341\n",
      "Test evaluation:\n",
      "[0.6869243383407593, 11.530879020690918, 0.9583535194396973, 0.39243364334106445, 11.530879020690918, 0.6618398427963257, 0.834114134311676]\n",
      "\n",
      "------Session 28------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\28\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6401 - age_dense_out_loss: 12.2761 - ethnicity_dense_out_loss: 0.9010 - gender_dense_out_loss: 0.3545 - age_dense_out_mae: 12.2761 - ethnicity_dense_out_accuracy: 0.6865 - gender_dense_out_accuracy: 0.8467\n",
      "Test evaluation:\n",
      "[0.6400666832923889, 12.276141166687012, 0.9010427594184875, 0.3545386791229248, 12.276141166687012, 0.686541736125946, 0.8466780185699463]\n",
      "\n",
      "------Session 29------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\29\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5943 - age_dense_out_loss: 10.9188 - ethnicity_dense_out_loss: 0.8149 - gender_dense_out_loss: 0.3518 - age_dense_out_mae: 10.9188 - ethnicity_dense_out_accuracy: 0.7204 - gender_dense_out_accuracy: 0.8501\n",
      "Test evaluation:\n",
      "[0.5942716002464294, 10.918806076049805, 0.8148570656776428, 0.35184788703918457, 10.918806076049805, 0.720400333404541, 0.8500851988792419]\n",
      "\n",
      "------Session 30------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\30\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5820 - age_dense_out_loss: 11.2214 - ethnicity_dense_out_loss: 0.8176 - gender_dense_out_loss: 0.3240 - age_dense_out_mae: 11.2214 - ethnicity_dense_out_accuracy: 0.7208 - gender_dense_out_accuracy: 0.8614\n",
      "Test evaluation:\n",
      "[0.5820035934448242, 11.221379280090332, 0.8175585269927979, 0.3240056335926056, 11.221379280090332, 0.7208262085914612, 0.8613713979721069]\n",
      "\n",
      "------Session 31------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\31\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5897 - age_dense_out_loss: 11.0461 - ethnicity_dense_out_loss: 0.8230 - gender_dense_out_loss: 0.3342 - age_dense_out_mae: 11.0461 - ethnicity_dense_out_accuracy: 0.7181 - gender_dense_out_accuracy: 0.8546\n",
      "Test evaluation:\n",
      "[0.5896621942520142, 11.046131134033203, 0.823043942451477, 0.3341887593269348, 11.046131134033203, 0.7180579304695129, 0.8545570969581604]\n",
      "\n",
      "------Session 32------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\32\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5917 - age_dense_out_loss: 11.2357 - ethnicity_dense_out_loss: 0.8264 - gender_dense_out_loss: 0.3346 - age_dense_out_mae: 11.2357 - ethnicity_dense_out_accuracy: 0.7140 - gender_dense_out_accuracy: 0.8582\n",
      "Test evaluation:\n",
      "[0.5917260050773621, 11.235718727111816, 0.8263844847679138, 0.3345969617366791, 11.235718727111816, 0.7140119075775146, 0.8581771850585938]\n",
      "\n",
      "------Session 33------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\33\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5800 - age_dense_out_loss: 10.4519 - ethnicity_dense_out_loss: 0.8043 - gender_dense_out_loss: 0.3348 - age_dense_out_mae: 10.4519 - ethnicity_dense_out_accuracy: 0.7213 - gender_dense_out_accuracy: 0.8554\n",
      "Test evaluation:\n",
      "[0.5799857974052429, 10.451930046081543, 0.8042978644371033, 0.3347697854042053, 10.451930046081543, 0.7212521433830261, 0.8554088473320007]\n",
      "\n",
      "------Session 34------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\34\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5800 - age_dense_out_loss: 10.6411 - ethnicity_dense_out_loss: 0.8102 - gender_dense_out_loss: 0.3284 - age_dense_out_mae: 10.6411 - ethnicity_dense_out_accuracy: 0.7221 - gender_dense_out_accuracy: 0.8571\n",
      "Test evaluation:\n",
      "[0.5799700021743774, 10.641084671020508, 0.8102294206619263, 0.3284287750720978, 10.641084671020508, 0.7221038937568665, 0.8571124076843262]\n",
      "\n",
      "------Session 35------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\35\\assets\n",
      "294/294 [==============================] - 1s 5ms/step - loss: 0.5534 - age_dense_out_loss: 9.7028 - ethnicity_dense_out_loss: 0.7717 - gender_dense_out_loss: 0.3156 - age_dense_out_mae: 9.7028 - ethnicity_dense_out_accuracy: 0.7400 - gender_dense_out_accuracy: 0.8597\n",
      "Test evaluation:\n",
      "[0.553395688533783, 9.702783584594727, 0.7717382311820984, 0.3156474828720093, 9.702783584594727, 0.7399914860725403, 0.8596677780151367]\n",
      "\n",
      "------Session 36------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\36\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6875 - age_dense_out_loss: 13.0007 - ethnicity_dense_out_loss: 0.9807 - gender_dense_out_loss: 0.3684 - age_dense_out_mae: 13.0007 - ethnicity_dense_out_accuracy: 0.6418 - gender_dense_out_accuracy: 0.8392\n",
      "Test evaluation:\n",
      "[0.6875346302986145, 13.000654220581055, 0.9806548953056335, 0.3684127926826477, 13.000654220581055, 0.6418228149414062, 0.8392248749732971]\n",
      "\n",
      "------Session 37------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\37\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.7255 - age_dense_out_loss: 13.0004 - ethnicity_dense_out_loss: 0.9977 - gender_dense_out_loss: 0.4273 - age_dense_out_mae: 13.0004 - ethnicity_dense_out_accuracy: 0.6399 - gender_dense_out_accuracy: 0.8066\n",
      "Test evaluation:\n",
      "[0.7254944443702698, 13.000432014465332, 0.9977163076400757, 0.4272725582122803, 13.000432014465332, 0.6399062871932983, 0.8066439628601074]\n",
      "\n",
      "------Session 38------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\38\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.7567 - age_dense_out_loss: 13.2145 - ethnicity_dense_out_loss: 1.0386 - gender_dense_out_loss: 0.4484 - age_dense_out_mae: 13.2145 - ethnicity_dense_out_accuracy: 0.6167 - gender_dense_out_accuracy: 0.7926\n",
      "Test evaluation:\n",
      "[0.7566810846328735, 13.214493751525879, 1.038560152053833, 0.44837361574172974, 13.214493751525879, 0.6166950464248657, 0.7925894260406494]\n",
      "\n",
      "------Session 39------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\39\\assets\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.7128 - age_dense_out_loss: 12.8493 - ethnicity_dense_out_loss: 0.9628 - gender_dense_out_loss: 0.4371 - age_dense_out_mae: 12.8493 - ethnicity_dense_out_accuracy: 0.6618 - gender_dense_out_accuracy: 0.7881\n",
      "Test evaluation:\n",
      "[0.7127729654312134, 12.849334716796875, 0.9627752304077148, 0.43707218766212463, 12.849334716796875, 0.6618398427963257, 0.788117527961731]\n",
      "\n",
      "------Session 40------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\40\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.7010 - age_dense_out_loss: 11.9841 - ethnicity_dense_out_loss: 0.9903 - gender_dense_out_loss: 0.3876 - age_dense_out_mae: 11.9841 - ethnicity_dense_out_accuracy: 0.6493 - gender_dense_out_accuracy: 0.8313\n",
      "Test evaluation:\n",
      "[0.7009665966033936, 11.984082221984863, 0.9903185963630676, 0.3876466155052185, 11.984082221984863, 0.6492759585380554, 0.831345796585083]\n",
      "\n",
      "------Session 41------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\41\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6544 - age_dense_out_loss: 11.9264 - ethnicity_dense_out_loss: 0.9272 - gender_dense_out_loss: 0.3577 - age_dense_out_mae: 11.9264 - ethnicity_dense_out_accuracy: 0.6759 - gender_dense_out_accuracy: 0.8465\n",
      "Test evaluation:\n",
      "[0.6543588638305664, 11.926362991333008, 0.9271774888038635, 0.3576869070529938, 11.926362991333008, 0.6758943796157837, 0.8464650511741638]\n",
      "\n",
      "------Session 42------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\42\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6960 - age_dense_out_loss: 12.0159 - ethnicity_dense_out_loss: 1.0103 - gender_dense_out_loss: 0.3577 - age_dense_out_mae: 12.0159 - ethnicity_dense_out_accuracy: 0.6425 - gender_dense_out_accuracy: 0.8458\n",
      "Test evaluation:\n",
      "[0.6960219144821167, 12.015892028808594, 1.0102986097335815, 0.3577139675617218, 12.015892028808594, 0.6424616575241089, 0.8458262085914612]\n",
      "\n",
      "------Session 43------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\43\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6280 - age_dense_out_loss: 12.0965 - ethnicity_dense_out_loss: 0.8835 - gender_dense_out_loss: 0.3484 - age_dense_out_mae: 12.0965 - ethnicity_dense_out_accuracy: 0.6953 - gender_dense_out_accuracy: 0.8482\n",
      "Test evaluation:\n",
      "[0.6280110478401184, 12.096455574035645, 0.8834675550460815, 0.3483618497848511, 12.096455574035645, 0.6952725648880005, 0.848168671131134]\n",
      "\n",
      "------Session 44------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\44\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.7421 - age_dense_out_loss: 12.4046 - ethnicity_dense_out_loss: 0.9998 - gender_dense_out_loss: 0.4596 - age_dense_out_mae: 12.4046 - ethnicity_dense_out_accuracy: 0.6352 - gender_dense_out_accuracy: 0.7898\n",
      "Test evaluation:\n",
      "[0.7421382665634155, 12.404550552368164, 0.9998367428779602, 0.45963138341903687, 12.404550552368164, 0.6352214813232422, 0.7898211479187012]\n",
      "\n",
      "------Session 45------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\45\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6006 - age_dense_out_loss: 11.6252 - ethnicity_dense_out_loss: 0.8408 - gender_dense_out_loss: 0.3371 - age_dense_out_mae: 11.6252 - ethnicity_dense_out_accuracy: 0.7102 - gender_dense_out_accuracy: 0.8490\n",
      "Test evaluation:\n",
      "[0.6005925536155701, 11.625237464904785, 0.8407958745956421, 0.33713898062705994, 11.625237464904785, 0.7101788520812988, 0.8490204215049744]\n",
      "\n",
      "------Session 46------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\46\\assets\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\46\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\46\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6017 - age_dense_out_loss: 11.7842 - ethnicity_dense_out_loss: 0.8364 - gender_dense_out_loss: 0.3434 - age_dense_out_mae: 11.7842 - ethnicity_dense_out_accuracy: 0.7136 - gender_dense_out_accuracy: 0.8520\n",
      "Test evaluation:\n",
      "[0.6016721725463867, 11.784235954284668, 0.8363536596298218, 0.34342166781425476, 11.784235954284668, 0.7135860323905945, 0.8520017266273499]\n",
      "\n",
      "------Session 47------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\47\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5863 - age_dense_out_loss: 10.6271 - ethnicity_dense_out_loss: 0.8184 - gender_dense_out_loss: 0.3328 - age_dense_out_mae: 10.6271 - ethnicity_dense_out_accuracy: 0.7210 - gender_dense_out_accuracy: 0.8573\n",
      "Test evaluation:\n",
      "[0.5862512588500977, 10.627105712890625, 0.8184128999710083, 0.33283498883247375, 10.627105712890625, 0.7210391759872437, 0.8573253750801086]\n",
      "\n",
      "------Session 48------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\48\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6071 - age_dense_out_loss: 11.2790 - ethnicity_dense_out_loss: 0.8603 - gender_dense_out_loss: 0.3313 - age_dense_out_mae: 11.2790 - ethnicity_dense_out_accuracy: 0.6987 - gender_dense_out_accuracy: 0.8543\n",
      "Test evaluation:\n",
      "[0.6070573925971985, 11.278996467590332, 0.8603067994117737, 0.33125048875808716, 11.278996467590332, 0.6986797451972961, 0.8543441295623779]\n",
      "\n",
      "------Session 49------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\49\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6321 - age_dense_out_loss: 11.8788 - ethnicity_dense_out_loss: 0.8875 - gender_dense_out_loss: 0.3529 - age_dense_out_mae: 11.8788 - ethnicity_dense_out_accuracy: 0.6891 - gender_dense_out_accuracy: 0.8499\n",
      "Test evaluation:\n",
      "[0.6320684552192688, 11.878767013549805, 0.8875043988227844, 0.3528755307197571, 11.878767013549805, 0.6890971064567566, 0.8498722314834595]\n",
      "\n",
      "------Session 50------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\50\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5631 - age_dense_out_loss: 10.0602 - ethnicity_dense_out_loss: 0.7850 - gender_dense_out_loss: 0.3210 - age_dense_out_mae: 10.0602 - ethnicity_dense_out_accuracy: 0.7306 - gender_dense_out_accuracy: 0.8624\n",
      "Test evaluation:\n",
      "[0.5631013512611389, 10.060201644897461, 0.7850396633148193, 0.321042537689209, 10.060201644897461, 0.7306218147277832, 0.8624361157417297]\n",
      "\n",
      "------Session 51------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\51\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5915 - age_dense_out_loss: 11.2276 - ethnicity_dense_out_loss: 0.8297 - gender_dense_out_loss: 0.3309 - age_dense_out_mae: 11.2276 - ethnicity_dense_out_accuracy: 0.7183 - gender_dense_out_accuracy: 0.8580\n",
      "Test evaluation:\n",
      "[0.5915256142616272, 11.227635383605957, 0.8297204375267029, 0.33087658882141113, 11.227635383605957, 0.7182708978652954, 0.8579642176628113]\n",
      "\n",
      "------Session 52------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\52\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\52\\assets\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5693 - age_dense_out_loss: 10.2994 - ethnicity_dense_out_loss: 0.7949 - gender_dense_out_loss: 0.3230 - age_dense_out_mae: 10.2994 - ethnicity_dense_out_accuracy: 0.7317 - gender_dense_out_accuracy: 0.8639\n",
      "Test evaluation:\n",
      "[0.5692501068115234, 10.299388885498047, 0.794914722442627, 0.3229866325855255, 10.299388885498047, 0.731686532497406, 0.8639267683029175]\n",
      "\n",
      "------Session 53------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\53\\assets\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\53\\assets\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_210615\\models\\53\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6027 - age_dense_out_loss: 11.1055 - ethnicity_dense_out_loss: 0.8456 - gender_dense_out_loss: 0.3376 - age_dense_out_mae: 11.1055 - ethnicity_dense_out_accuracy: 0.7019 - gender_dense_out_accuracy: 0.8560\n",
      "Test evaluation:\n",
      "[0.6026802659034729, 11.105483055114746, 0.845554769039154, 0.33759504556655884, 11.105483055114746, 0.7018739581108093, 0.8560476899147034]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_fine_tune_dirs():\n",
    "    timestamp = datetime.datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "    dir = os.path.join(os.path.curdir, \"fine_tune\", timestamp)\n",
    "    Path(dir).mkdir(parents=True, exist_ok=True)\n",
    "    return dir\n",
    "\n",
    "dir = make_fine_tune_dirs()\n",
    "session = 0\n",
    "for bs in HP_BATCH_SIZE.domain.values:\n",
    "    for op in HP_OPTIMIZER.domain.values:\n",
    "        for ft in HP_FILTERS.domain.values:\n",
    "            for units in HP_UNITS.domain.values:\n",
    "                print(f\"------Session {session}------\")\n",
    "                hparams = {\n",
    "                    HP_BATCH_SIZE: bs,\n",
    "                    HP_OPTIMIZER: op,\n",
    "                    HP_FILTERS: ft,\n",
    "                    HP_UNITS: units\n",
    "                }\n",
    "                print({h.name: hparams[h] for h in hparams})\n",
    "                fine_tune_hparams(hparams, dir, session)\n",
    "                session += 1\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present your findings for 5 different runs by fine-tuning the hyperparameters. The results table must contain the following fields\n",
    "- A minimum of 5 hyperparameters that you have fine-tuned\n",
    "- Mean absolute error for age\n",
    "- Accuracy for ethnicity prediction\n",
    "- Accuracy for gender prediction\n",
    "Please use a table format similar to the one mentioned below when presenting the results.\n",
    "\n",
    "With the `Session id` below you can find the tensorboard logs and models in `./fine_tune/<time_stamp>` dir\n",
    "\n",
    "|Session id| Hyperparameters | Age(MAE) | Ethnicity(Accuracy)| Gender(Accuracy)|\n",
    "|-------|-----------------|----------|--------------------|------------------|\n",
    "|7|{'batch_size': 8, 'optimizer': 'Adam', 'units': 128}| 9.0172| 0.7481| 0.8754|\n",
    "|8|{'batch_size': 8, 'optimizer': 'Adam', 'units': 256}| 8.6618| 0.7594| 0.8644|\n",
    "|26|{'batch_size': 8, 'optimizer': 'RMSprop', 'units': 256}| 8.9216| 0.7500| 0.8703|\n",
    "|13|{'batch_size': 8, 'optimizer': 'Adamax', 'units': 128}| 10.9184| 0.7221| 0.8626|\n",
    "|16|{'batch_size': 8, 'optimizer': 'Adamax', 'units': 128}| 9.8076| 0.7351| 0.8620|\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
