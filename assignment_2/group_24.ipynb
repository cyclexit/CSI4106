{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI4106 Assignment 2\n",
    "\n",
    "## Group 24\n",
    "\n",
    "|Name|Student No.|Email\n",
    "|----|-----------|-----\n",
    "|Hongyi Lin| 300053082| hlin087@uottawa.ca\n",
    "|Rodger Retanal| 300052309| rreta014@uottawa.ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# for reproducibility purposes\n",
    "SEED = 123\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "# load tensorboard extension\n",
    "# %reload_ext tensorboard\n",
    "# specify the log directory where the tensorboard logs will be written\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the relevant datasets (15/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set.shape = (15026, 4)\n",
      "val_set.shape = (3757, 4)\n",
      "test_set.shape = (4696, 4)\n",
      "columns: Index(['age', 'ethnicity', 'gender', 'img_name'], dtype='object')\n",
      "\n",
      "gender distribution in train_set:\n",
      " 0    2456\n",
      "1    2240\n",
      "Name: gender, dtype: int64\n",
      "gender distribution in val_set:\n",
      " 0    1965\n",
      "1    1792\n",
      "Name: gender, dtype: int64\n",
      "gender distribution in test_set:\n",
      " 0    2456\n",
      "1    2240\n",
      "Name: gender, dtype: int64\n",
      "\n",
      "ethnicity distribution in train_set:\n",
      " 0    1991\n",
      "1     896\n",
      "3     790\n",
      "2     683\n",
      "4     336\n",
      "Name: ethnicity, dtype: int64\n",
      "ethnicity distribution in val_set:\n",
      " 0    1593\n",
      "1     717\n",
      "3     632\n",
      "2     547\n",
      "4     268\n",
      "Name: ethnicity, dtype: int64\n",
      "ethnicity distribution in test_set:\n",
      " 0    1991\n",
      "1     896\n",
      "3     790\n",
      "2     683\n",
      "4     336\n",
      "Name: ethnicity, dtype: int64 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the datasets using the csv files train, val and test \n",
    "# (3)\n",
    "train_set = pd.read_csv(\"./data/train.csv\")\n",
    "val_set = pd.read_csv(\"./data/val.csv\")\n",
    "test_set = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# print the shapes of the dataframes \n",
    "# (3)\n",
    "print(f\"train_set.shape = {train_set.shape}\")\n",
    "print(f\"val_set.shape = {val_set.shape}\")\n",
    "print(f\"test_set.shape = {test_set.shape}\")\n",
    "\n",
    "# print the column names from either one of the dataframes \n",
    "# (1)\n",
    "print(f\"columns: {train_set.columns}\\n\")\n",
    "\n",
    "# print the proportional distribution of gender in all three datasets(i.e., number of male and female) \n",
    "# (3)\n",
    "print(f\"gender distribution in train_set:\\n {test_set['gender'].value_counts()}\")\n",
    "print(f\"gender distribution in val_set:\\n {val_set['gender'].value_counts()}\")\n",
    "print(f\"gender distribution in test_set:\\n {test_set['gender'].value_counts()}\\n\")\n",
    "\n",
    "# print the proportional distribution of ethnicity in all three datasets \n",
    "# (3)\n",
    "print(f\"ethnicity distribution in train_set:\\n {test_set['ethnicity'].value_counts()}\")\n",
    "print(f\"ethnicity distribution in val_set:\\n {val_set['ethnicity'].value_counts()}\")\n",
    "print(f\"ethnicity distribution in test_set:\\n {test_set['ethnicity'].value_counts()} \")\n",
    "\n",
    "# plot the age distribution from the training dataset where the x-axis plots the age\n",
    "# and the y-axis depicts the count of individuals within each age group. For example, individuals with age=1 are:\n",
    "# (2)\n",
    "train_set[\"age\"].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ImageDataGenerators (22/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "(16, 48, 48, 1)\n",
      "(48, 48, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ImageDataGenerator is an iterator.\n",
    "\n",
    "# specify the batch size hyperparameter. You can experiment with different batch sizes\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# create the ImageDataGenerator with rescaling that will generate batched tensors representing images with real-time data augmentation\n",
    "# use at least two of the augmentation strategies. For example, fill_mode='nearest'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (3)\n",
    "train_img_gen = ImageDataGenerator(\n",
    "    rescale= 1.0 / 255,\n",
    "    fill_mode=\"constant\",\n",
    "    cval=0.0\n",
    ")\n",
    "\n",
    "# set up the x_col and y_col\n",
    "x_col = \"img_name\"\n",
    "y_col = list(train_set.columns)\n",
    "y_col.remove(x_col)\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance to link the image folder and the dataframe.\n",
    "# also include the, batch size, image size and the seed.\n",
    "# make sure to include the following arguments\n",
    "# color_mode='grayscale', class_mode='multi_output'\n",
    "# please refer: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "# (5)\n",
    "train_itr = train_img_gen.flow_from_dataframe(\n",
    "    dataframe=train_set,\n",
    "    directory=\"./data/images/train/\",\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(48, 48),\n",
    "    seed=SEED,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"multi_output\"\n",
    ")\n",
    "\n",
    "\n",
    "# similarly, create an ImageDataGenerator for the validation dataset and make sure not to use any of th eaugmentation strategies except rescaling the image\n",
    "# (2)\n",
    "val_img_gen = ImageDataGenerator(\n",
    "    rescale= 1.0 / 255\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the \"ImageDataGenerator\" instance with the same arguments as above\n",
    "# make sure to specify the following arguments:\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "val_itr = val_img_gen.flow_from_dataframe(\n",
    "    dataframe=val_set,\n",
    "    directory=\"./data/images/val/\",\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(48, 48),\n",
    "    seed=SEED,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"multi_output\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# use the method \"flow_from_dataframe\" from the val_img_gen instance to link the test dataframe and the test data folder\n",
    "# In addition, make sure to specify the following arguments\n",
    "# class_mode='multi_output', color_mode='grayscale', shuffle=False\n",
    "# (5)\n",
    "test_itr = val_img_gen.flow_from_dataframe(\n",
    "    dataframe=test_set,\n",
    "    directory=\"./data/images/test/\",\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=(48, 48),\n",
    "    seed=SEED,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"multi_output\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# enumerate through the validation data generator created above and plot first grayscale image \n",
    "# (2)\n",
    "for i, element in enumerate(val_itr):\n",
    "    print(element[0].shape)\n",
    "    tmp = element[0][0]\n",
    "    print(tmp.shape)\n",
    "    plt.imshow(tmp, cmap=plt.cm.binary)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model (44/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"group_24_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 48, 48, 8)    80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 24, 24, 8)    0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4608)         0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "age_dense_1 (Dense)             (None, 128)          589952      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ethnicity_dense_1 (Dense)       (None, 128)          589952      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gender_dense_1 (Dense)          (None, 128)          589952      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "age_dense_out (Dense)           (None, 1)            129         age_dense_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ethnicity_dense_out (Dense)     (None, 5)            645         ethnicity_dense_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "gender_dense_out (Dense)        (None, 1)            129         gender_dense_1[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,770,839\n",
      "Trainable params: 1,770,839\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "len(train_itr)=940\n",
      "GPU=[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "is_built_with_cuda=True\n",
      "<class 'tensorflow.python.keras.preprocessing.image.DataFrameIterator'>\n",
      "Epoch 1/20\n",
      "58/58 [==============================] - 4s 17ms/step - loss: 1.1781 - age_dense_out_loss: 17.1503 - ethnicity_dense_out_loss: 1.5867 - gender_dense_out_loss: 0.7351 - age_dense_out_mae: 17.1503 - ethnicity_dense_out_accuracy: 0.3761 - gender_dense_out_accuracy: 0.6028 - val_loss: 1.0451 - val_age_dense_out_loss: 17.9068 - val_ethnicity_dense_out_loss: 1.4523 - val_gender_dense_out_loss: 0.6020 - val_age_dense_out_mae: 17.9068 - val_ethnicity_dense_out_accuracy: 0.4458 - val_gender_dense_out_accuracy: 0.6792\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 2/20\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.9953 - age_dense_out_loss: 16.8528 - ethnicity_dense_out_loss: 1.3469 - gender_dense_out_loss: 0.6101 - age_dense_out_mae: 16.8528 - ethnicity_dense_out_accuracy: 0.4883 - gender_dense_out_accuracy: 0.6716 - val_loss: 0.9548 - val_age_dense_out_loss: 17.5145 - val_ethnicity_dense_out_loss: 1.2335 - val_gender_dense_out_loss: 0.6411 - val_age_dense_out_mae: 17.5145 - val_ethnicity_dense_out_accuracy: 0.5083 - val_gender_dense_out_accuracy: 0.5833\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 3/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.9048 - age_dense_out_loss: 15.8431 - ethnicity_dense_out_loss: 1.2272 - gender_dense_out_loss: 0.5507 - age_dense_out_mae: 15.8431 - ethnicity_dense_out_accuracy: 0.5297 - gender_dense_out_accuracy: 0.7299 - val_loss: 0.8421 - val_age_dense_out_loss: 16.4651 - val_ethnicity_dense_out_loss: 1.1555 - val_gender_dense_out_loss: 0.4958 - val_age_dense_out_mae: 16.4651 - val_ethnicity_dense_out_accuracy: 0.5625 - val_gender_dense_out_accuracy: 0.7667\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 4/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8613 - age_dense_out_loss: 15.5014 - ethnicity_dense_out_loss: 1.1850 - gender_dense_out_loss: 0.5065 - age_dense_out_mae: 15.5014 - ethnicity_dense_out_accuracy: 0.5530 - gender_dense_out_accuracy: 0.7500 - val_loss: 0.8057 - val_age_dense_out_loss: 15.6363 - val_ethnicity_dense_out_loss: 1.1582 - val_gender_dense_out_loss: 0.4220 - val_age_dense_out_mae: 15.6363 - val_ethnicity_dense_out_accuracy: 0.5583 - val_gender_dense_out_accuracy: 0.8208\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 5/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8040 - age_dense_out_loss: 14.7927 - ethnicity_dense_out_loss: 1.1037 - gender_dense_out_loss: 0.4746 - age_dense_out_mae: 14.7927 - ethnicity_dense_out_accuracy: 0.5953 - gender_dense_out_accuracy: 0.7765 - val_loss: 0.7452 - val_age_dense_out_loss: 13.9789 - val_ethnicity_dense_out_loss: 1.0597 - val_gender_dense_out_loss: 0.4029 - val_age_dense_out_mae: 13.9789 - val_ethnicity_dense_out_accuracy: 0.5792 - val_gender_dense_out_accuracy: 0.8292\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 6/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7445 - age_dense_out_loss: 13.9881 - ethnicity_dense_out_loss: 1.0393 - gender_dense_out_loss: 0.4217 - age_dense_out_mae: 13.9881 - ethnicity_dense_out_accuracy: 0.6144 - gender_dense_out_accuracy: 0.8051 - val_loss: 0.7269 - val_age_dense_out_loss: 13.2436 - val_ethnicity_dense_out_loss: 1.0610 - val_gender_dense_out_loss: 0.3663 - val_age_dense_out_mae: 13.2436 - val_ethnicity_dense_out_accuracy: 0.6292 - val_gender_dense_out_accuracy: 0.8333\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 7/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7086 - age_dense_out_loss: 14.1733 - ethnicity_dense_out_loss: 0.9659 - gender_dense_out_loss: 0.4229 - age_dense_out_mae: 14.1733 - ethnicity_dense_out_accuracy: 0.6568 - gender_dense_out_accuracy: 0.8104 - val_loss: 0.6879 - val_age_dense_out_loss: 12.8098 - val_ethnicity_dense_out_loss: 0.9839 - val_gender_dense_out_loss: 0.3663 - val_age_dense_out_mae: 12.8098 - val_ethnicity_dense_out_accuracy: 0.6167 - val_gender_dense_out_accuracy: 0.8625\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 8/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6873 - age_dense_out_loss: 12.8380 - ethnicity_dense_out_loss: 0.9527 - gender_dense_out_loss: 0.3961 - age_dense_out_mae: 12.8380 - ethnicity_dense_out_accuracy: 0.6409 - gender_dense_out_accuracy: 0.8305 - val_loss: 0.6778 - val_age_dense_out_loss: 12.4076 - val_ethnicity_dense_out_loss: 0.9826 - val_gender_dense_out_loss: 0.3482 - val_age_dense_out_mae: 12.4076 - val_ethnicity_dense_out_accuracy: 0.6292 - val_gender_dense_out_accuracy: 0.8417\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 9/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6763 - age_dense_out_loss: 12.7978 - ethnicity_dense_out_loss: 0.9405 - gender_dense_out_loss: 0.3864 - age_dense_out_mae: 12.7978 - ethnicity_dense_out_accuracy: 0.6631 - gender_dense_out_accuracy: 0.8157 - val_loss: 0.6801 - val_age_dense_out_loss: 11.9346 - val_ethnicity_dense_out_loss: 0.9778 - val_gender_dense_out_loss: 0.3584 - val_age_dense_out_mae: 11.9346 - val_ethnicity_dense_out_accuracy: 0.6250 - val_gender_dense_out_accuracy: 0.8292\n",
      "Epoch 10/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6696 - age_dense_out_loss: 12.6239 - ethnicity_dense_out_loss: 0.9252 - gender_dense_out_loss: 0.3888 - age_dense_out_mae: 12.6239 - ethnicity_dense_out_accuracy: 0.6748 - gender_dense_out_accuracy: 0.8263 - val_loss: 0.6423 - val_age_dense_out_loss: 11.9694 - val_ethnicity_dense_out_loss: 0.9040 - val_gender_dense_out_loss: 0.3567 - val_age_dense_out_mae: 11.9694 - val_ethnicity_dense_out_accuracy: 0.6667 - val_gender_dense_out_accuracy: 0.8333\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 11/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6690 - age_dense_out_loss: 12.3886 - ethnicity_dense_out_loss: 0.9262 - gender_dense_out_loss: 0.3870 - age_dense_out_mae: 12.3886 - ethnicity_dense_out_accuracy: 0.6653 - gender_dense_out_accuracy: 0.8252 - val_loss: 0.6450 - val_age_dense_out_loss: 11.7139 - val_ethnicity_dense_out_loss: 0.9229 - val_gender_dense_out_loss: 0.3437 - val_age_dense_out_mae: 11.7139 - val_ethnicity_dense_out_accuracy: 0.6667 - val_gender_dense_out_accuracy: 0.8542\n",
      "Epoch 12/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6057 - age_dense_out_loss: 11.7317 - ethnicity_dense_out_loss: 0.8516 - gender_dense_out_loss: 0.3363 - age_dense_out_mae: 11.7317 - ethnicity_dense_out_accuracy: 0.7140 - gender_dense_out_accuracy: 0.8559 - val_loss: 0.6431 - val_age_dense_out_loss: 11.0694 - val_ethnicity_dense_out_loss: 0.9185 - val_gender_dense_out_loss: 0.3456 - val_age_dense_out_mae: 11.0694 - val_ethnicity_dense_out_accuracy: 0.6792 - val_gender_dense_out_accuracy: 0.8583\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5693 - age_dense_out_loss: 11.8581 - ethnicity_dense_out_loss: 0.7934 - gender_dense_out_loss: 0.3216 - age_dense_out_mae: 11.8581 - ethnicity_dense_out_accuracy: 0.7320 - gender_dense_out_accuracy: 0.8644 - val_loss: 0.5929 - val_age_dense_out_loss: 10.7419 - val_ethnicity_dense_out_loss: 0.8442 - val_gender_dense_out_loss: 0.3202 - val_age_dense_out_mae: 10.7419 - val_ethnicity_dense_out_accuracy: 0.7000 - val_gender_dense_out_accuracy: 0.8625\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 14/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5802 - age_dense_out_loss: 10.8494 - ethnicity_dense_out_loss: 0.8180 - gender_dense_out_loss: 0.3206 - age_dense_out_mae: 10.8494 - ethnicity_dense_out_accuracy: 0.7182 - gender_dense_out_accuracy: 0.8623 - val_loss: 0.5879 - val_age_dense_out_loss: 10.6640 - val_ethnicity_dense_out_loss: 0.8383 - val_gender_dense_out_loss: 0.3162 - val_age_dense_out_mae: 10.6640 - val_ethnicity_dense_out_accuracy: 0.7083 - val_gender_dense_out_accuracy: 0.8667\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 15/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5662 - age_dense_out_loss: 10.5009 - ethnicity_dense_out_loss: 0.7833 - gender_dense_out_loss: 0.3282 - age_dense_out_mae: 10.5009 - ethnicity_dense_out_accuracy: 0.7278 - gender_dense_out_accuracy: 0.8602 - val_loss: 0.5916 - val_age_dense_out_loss: 10.6486 - val_ethnicity_dense_out_loss: 0.8465 - val_gender_dense_out_loss: 0.3154 - val_age_dense_out_mae: 10.6486 - val_ethnicity_dense_out_accuracy: 0.7125 - val_gender_dense_out_accuracy: 0.8667\n",
      "Epoch 16/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5707 - age_dense_out_loss: 11.2358 - ethnicity_dense_out_loss: 0.8080 - gender_dense_out_loss: 0.3109 - age_dense_out_mae: 11.2358 - ethnicity_dense_out_accuracy: 0.6960 - gender_dense_out_accuracy: 0.8665 - val_loss: 0.5870 - val_age_dense_out_loss: 10.6161 - val_ethnicity_dense_out_loss: 0.8354 - val_gender_dense_out_loss: 0.3173 - val_age_dense_out_mae: 10.6161 - val_ethnicity_dense_out_accuracy: 0.7042 - val_gender_dense_out_accuracy: 0.8542\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 17/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5525 - age_dense_out_loss: 11.6291 - ethnicity_dense_out_loss: 0.7666 - gender_dense_out_loss: 0.3151 - age_dense_out_mae: 11.6291 - ethnicity_dense_out_accuracy: 0.7341 - gender_dense_out_accuracy: 0.8549 - val_loss: 0.5874 - val_age_dense_out_loss: 10.7139 - val_ethnicity_dense_out_loss: 0.8399 - val_gender_dense_out_loss: 0.3134 - val_age_dense_out_mae: 10.7139 - val_ethnicity_dense_out_accuracy: 0.7083 - val_gender_dense_out_accuracy: 0.8667\n",
      "Epoch 18/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5580 - age_dense_out_loss: 11.7476 - ethnicity_dense_out_loss: 0.7763 - gender_dense_out_loss: 0.3162 - age_dense_out_mae: 11.7476 - ethnicity_dense_out_accuracy: 0.7225 - gender_dense_out_accuracy: 0.8739 - val_loss: 0.5831 - val_age_dense_out_loss: 10.6461 - val_ethnicity_dense_out_loss: 0.8324 - val_gender_dense_out_loss: 0.3125 - val_age_dense_out_mae: 10.6461 - val_ethnicity_dense_out_accuracy: 0.7083 - val_gender_dense_out_accuracy: 0.8708\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n",
      "Epoch 19/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5683 - age_dense_out_loss: 11.4491 - ethnicity_dense_out_loss: 0.7816 - gender_dense_out_loss: 0.3322 - age_dense_out_mae: 11.4491 - ethnicity_dense_out_accuracy: 0.7352 - gender_dense_out_accuracy: 0.8559 - val_loss: 0.5877 - val_age_dense_out_loss: 10.7900 - val_ethnicity_dense_out_loss: 0.8400 - val_gender_dense_out_loss: 0.3137 - val_age_dense_out_mae: 10.7900 - val_ethnicity_dense_out_accuracy: 0.7042 - val_gender_dense_out_accuracy: 0.8583\n",
      "Epoch 20/20\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6202 - age_dense_out_loss: 11.7731 - ethnicity_dense_out_loss: 0.8819 - gender_dense_out_loss: 0.3350 - age_dense_out_mae: 11.7731 - ethnicity_dense_out_accuracy: 0.7044 - gender_dense_out_accuracy: 0.8591 - val_loss: 0.5776 - val_age_dense_out_loss: 10.6209 - val_ethnicity_dense_out_loss: 0.8222 - val_gender_dense_out_loss: 0.3117 - val_age_dense_out_mae: 10.6209 - val_ethnicity_dense_out_accuracy: 0.7167 - val_gender_dense_out_accuracy: 0.8750\n",
      "INFO:tensorflow:Assets written to: .\\models\\20220401_223022\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1daa9ee20d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify the model input with the required shape \n",
    "# (1)\n",
    "inputs = layers.Input(shape=(48, 48, 1))\n",
    "\n",
    "# The shared layers\n",
    "# Include at least one Conv2D layer, MaxPooling2D layer and a Flatten layer\n",
    "# you can have as many layers as possible, but make sure not to overfit your model using the training data\n",
    "# (10)\n",
    "shared_layers = layers.Conv2D(\n",
    "    filters=8,\n",
    "    kernel_size=3,\n",
    "    activation=\"relu\",\n",
    "    strides=(1, 1),\n",
    "    dilation_rate=(1, 1),\n",
    "    padding=\"same\"\n",
    ")(inputs)\n",
    "shared_layers = layers.MaxPooling2D(\n",
    "    pool_size=(2, 2)\n",
    ")(shared_layers)\n",
    "shared_layers = layers.Flatten()(shared_layers)\n",
    "\n",
    "# Task specific layers\n",
    "# Include at least one Dense layer as a task specific layer before generating the output for age\n",
    "# (2)\n",
    "task_layers_age = layers.Dense(units=128, activation=\"relu\", name=\"age_dense_1\")(shared_layers)\n",
    "\n",
    "# Include the age output and make sure to include the following arguments\n",
    "# activation='linear', name='xxx'(any name)\n",
    "# make sure to name your output layers so that different metrics to be used can be linked accordingly\n",
    "# please note that the age prediction is a regression task\n",
    "# (2)\n",
    "AGE_OUT = \"age_dense_out\"\n",
    "outputs_age = layers.Dense(units=1, activation=\"linear\", name=AGE_OUT)(task_layers_age) # NOTE: regression units=1\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for ethnicity prediction\n",
    "# (2)\n",
    "task_layers_ethnicity = layers.Dense(units=128, activation=\"relu\", name=\"ethnicity_dense_1\")(shared_layers)\n",
    "\n",
    "# Include the ethnicity output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a multi-class classification task\n",
    "# (2)\n",
    "ETHNICITY_OUT = \"ethnicity_dense_out\"\n",
    "outputs_ethnicity = layers.Dense(units=5, activation=\"softmax\", name=ETHNICITY_OUT)(task_layers_ethnicity) # NOTE: multi-class classification, units=num of classes\n",
    "\n",
    "# Similar to above, specify one or more Dense layers as task specific layers for gender prediction\n",
    "# (2)\n",
    "task_layers_gender = layers.Dense(units=128, activation=\"relu\", name=\"gender_dense_1\")(shared_layers) # NOTE: task layers unit can be more than 1\n",
    "\n",
    "# Include the gender output that uses the task specific output from the layer above\n",
    "# please note that the ethnicity prediction is a binary classification task\n",
    "# (2)\n",
    "GENDER_OUT = \"gender_dense_out\"\n",
    "outputs_gender = layers.Dense(units=1, activation=\"sigmoid\", name=GENDER_OUT)(task_layers_gender) # NOTE: sigmoid, units=1 only\n",
    "\n",
    "# create the model with the required input and the outputs.\n",
    "# please make sure that the outputs can be included in a list and make sure to keep note of the order\n",
    "# (3)\n",
    "outputs_list = [outputs_age, outputs_ethnicity, outputs_gender]\n",
    "outputs_idx = {\n",
    "    \"age\": 0,\n",
    "    \"ethnicity\": 1,\n",
    "    \"gender\": 2\n",
    "}\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs_list, name=\"group_24_model\")\n",
    "\n",
    "# print the model summary\n",
    "# (0.5)\n",
    "print(model.summary())\n",
    "\n",
    "# Instantiate the optimizer with the learning rate. You can start with the learning rate 1e-3(0.001).\n",
    "# Both the optimizer and the learning rate are hyperparameters that you can finetune\n",
    "# For example, you can start with the \"RMSprop\" optimizer\n",
    "# (2)\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "\n",
    "# specify the losses to be used for each task: age, ethnicity and gender prediction \n",
    "# (0.5)\n",
    "LOSSES = {\n",
    "    AGE_OUT: \"mae\",\n",
    "    ETHNICITY_OUT: \"sparse_categorical_crossentropy\",\n",
    "    GENDER_OUT: \"binary_crossentropy\"\n",
    "}\n",
    "\n",
    "# compile the model with the optimizer, loss, loss_weights and the metrics for each task\n",
    "# apply the following weights to the losses to balance the contribution of each loss to the total loss\n",
    "# loss_weights=[0.001, 0.5, 0.5]\n",
    "# please remember to use the relevant metric for each task by assigning it to the correct output\n",
    "# (2)\n",
    "LOSS_WEIGHTS = {\n",
    "    AGE_OUT: 0.001,\n",
    "    ETHNICITY_OUT: 0.5,\n",
    "    GENDER_OUT: 0.5\n",
    "}\n",
    "METRICS = {\n",
    "    AGE_OUT: \"mae\",\n",
    "    ETHNICITY_OUT: \"accuracy\",\n",
    "    GENDER_OUT: \"accuracy\"\n",
    "}\n",
    "model.compile(optimizer=optimizer, loss=LOSSES, loss_weights=LOSS_WEIGHTS, metrics=METRICS)\n",
    "\n",
    "# Define the callbacks\n",
    "# EarlyStopping: monitor the validation loss while waiting for 3 epochs before stopping\n",
    "# can restore the best weights\n",
    "# (2)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# create dirs for the checkpoint and tensorboard\n",
    "def make_dirs():\n",
    "    d = datetime.datetime.today()\n",
    "    timestamp = d.strftime('%Y%m%d_%H%M%S')\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(os.path.curdir, 'logs', timestamp)\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(os.path.curdir, 'models', timestamp)\n",
    "\n",
    "    Path(tensorlog_folder).mkdir(exist_ok=True, parents=True)\n",
    "    Path(checkpoint_folder).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder\n",
    "\n",
    "check_dir, tboard_dir = make_dirs()\n",
    "\n",
    "# ModelCheckpoint\n",
    "# monitor validation loss and save the best model weights\n",
    "# (2)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=check_dir,\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "# Initiallize TensorBoard\n",
    "# (2)\n",
    "tboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir = tboard_dir\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau\n",
    "# reduce the learning rate by a factor of 0.1 after waiting for 2 epochs while monitoring validation loss\n",
    "# specify a minimum learning rate to be used\n",
    "# (2)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    min_lr=5e-5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# fit the model with training and validation generators\n",
    "# In addition please specify the following arguments\n",
    "# steps_per_epoch=len(df_train)/batch_size\n",
    "# validation_steps=len(df_val)/batch_size\n",
    "# (5)\n",
    "print(f\"len(train_itr)={len(train_itr)}\")\n",
    "print(f\"GPU={tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"is_built_with_cuda={tf.test.is_built_with_cuda()}\")\n",
    "print(type(train_itr))\n",
    "model.fit(\n",
    "    train_itr,\n",
    "    validation_data=val_itr,\n",
    "    steps_per_epoch=len(train_itr) / BATCH_SIZE,\n",
    "    validation_steps=len(val_itr) / BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    shuffle=True,\n",
    "    callbacks=[reduce_lr, early_stop, checkpoint, tboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions on test data (14/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5984 - age_dense_out_loss: 11.4120 - ethnicity_dense_out_loss: 0.8353 - gender_dense_out_loss: 0.3387 - age_dense_out_mae: 11.4120 - ethnicity_dense_out_accuracy: 0.7093 - gender_dense_out_accuracy: 0.8560\n",
      "\n",
      "Test evaluation:\n",
      "[0.5983874201774597, 11.411970138549805, 0.8352797031402588, 0.3386712670326233, 11.411970138549805, 0.7093271017074585, 0.8560476899147034]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trained model using the test generator\n",
    "# print only the test accuracy for ethnicity and gender predictions\n",
    "# (4)\n",
    "test_evals = model.evaluate(test_itr, verbose=1)\n",
    "print(f\"\\nTest evaluation:\\n{test_evals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 1s 4ms/step\n",
      "Ethnicity Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.83      0.78      1991\n",
      "           1       0.73      0.79      0.76       896\n",
      "           2       0.73      0.69      0.71       683\n",
      "           3       0.64      0.61      0.62       790\n",
      "           4       0.35      0.04      0.06       336\n",
      "\n",
      "    accuracy                           0.71      4696\n",
      "   macro avg       0.64      0.59      0.59      4696\n",
      "weighted avg       0.68      0.71      0.69      4696\n",
      "\n",
      "Gender Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      2456\n",
      "           1       0.84      0.86      0.85      2240\n",
      "\n",
      "    accuracy                           0.86      4696\n",
      "   macro avg       0.86      0.86      0.86      4696\n",
      "weighted avg       0.86      0.86      0.86      4696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate predictions using the test generator\n",
    "# (2)\n",
    "predictions = model.predict(test_itr, verbose=1)\n",
    "# print(f\"\\nPredictions:\\n{predictions}\")\n",
    "\n",
    "# extract the ethnicity predictions\n",
    "# (2)\n",
    "# NOTE: we can only extract by remembering the order? 2nd is ethnicity\n",
    "predictions_ethnicity = np.argmax(predictions[outputs_idx[\"ethnicity\"]], axis=1)\n",
    "# print(predictions_ethnicity)\n",
    "\n",
    "# print the classification report for predicting ethnicity\n",
    "# (2)\n",
    "cr_ethnicity = classification_report(test_set[\"ethnicity\"].values, predictions_ethnicity)\n",
    "print(f\"Ethnicity Classification Report:\\n{cr_ethnicity}\")\n",
    "\n",
    "# extract the gender predictions where probabilities above 0.5 are considered class 1 and if not, class 0\n",
    "# (2)\n",
    "predictions_gender = predictions[outputs_idx[\"gender\"]].flatten().tolist()\n",
    "for i in range(0, len(predictions_gender)):\n",
    "    predictions_gender[i] = 1 if predictions_gender[i] > 0.5 else 0\n",
    "# print(predictions_gender)\n",
    "\n",
    "# print the classification report for predicting gender\n",
    "# (2)\n",
    "cr_gender = classification_report(test_set[\"gender\"].values, predictions_gender)\n",
    "print(f\"Gender Classification Report:\\n{cr_gender}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Present prediction results on test data(5/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "HP_BATCH_SIZE = hp.HParam(\"batch_size\", hp.Discrete([8, 16]))\n",
    "HP_OPTIMIZER = hp.HParam(\"optimizer\", hp.Discrete([\"RMSprop\", \"Adam\", \"Adamax\"]))\n",
    "HP_FILTERS = hp.HParam(\"filters\", hp.Discrete([8, 16, 32]))\n",
    "HP_UNITS = hp.HParam(\"units\", hp.Discrete([64, 128, 256]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_itr(bactch_size: int):\n",
    "    train_itr = train_img_gen.flow_from_dataframe(\n",
    "        dataframe=train_set,\n",
    "        directory=\"./data/images/train/\",\n",
    "        x_col=x_col,\n",
    "        y_col=y_col,\n",
    "        batch_size=bactch_size,\n",
    "        target_size=(48, 48),\n",
    "        seed=SEED,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=\"multi_output\"\n",
    "    )\n",
    "    val_itr = val_img_gen.flow_from_dataframe(\n",
    "        dataframe=val_set,\n",
    "        directory=\"./data/images/val/\",\n",
    "        x_col=x_col,\n",
    "        y_col=y_col,\n",
    "        batch_size=bactch_size,\n",
    "        target_size=(48, 48),\n",
    "        seed=SEED,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=\"multi_output\",\n",
    "        shuffle=False\n",
    "    )\n",
    "    test_itr = val_img_gen.flow_from_dataframe(\n",
    "        dataframe=test_set,\n",
    "        directory=\"./data/images/test/\",\n",
    "        x_col=x_col,\n",
    "        y_col=y_col,\n",
    "        batch_size=bactch_size,\n",
    "        target_size=(48, 48),\n",
    "        seed=SEED,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode=\"multi_output\",\n",
    "        shuffle=False\n",
    "    )\n",
    "    return {\"train\": train_itr, \"val\": val_itr, \"test\": test_itr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(filters, units, id):\n",
    "    # input\n",
    "    inputs = layers.Input(shape=(48, 48, 1))\n",
    "\n",
    "    # shared layers\n",
    "    shared_layers = layers.Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=3,\n",
    "        activation=\"relu\",\n",
    "        strides=(1, 1),\n",
    "        dilation_rate=(1, 1),\n",
    "        padding=\"same\"\n",
    "    )(inputs)\n",
    "    shared_layers = layers.MaxPooling2D(\n",
    "        pool_size=(2, 2)\n",
    "    )(shared_layers)\n",
    "    shared_layers = layers.Flatten()(shared_layers)\n",
    "\n",
    "    # task specific layers\n",
    "    task_layers_age = layers.Dense(units=units, activation=\"relu\", name=\"age_dense_1\")(shared_layers)\n",
    "    outputs_age = layers.Dense(units=1, activation=\"linear\", name=AGE_OUT)(task_layers_age)\n",
    "\n",
    "    task_layers_ethnicity = layers.Dense(units=units, activation=\"relu\", name=\"ethnicity_dense_1\")(shared_layers)\n",
    "    outputs_ethnicity = layers.Dense(units=5, activation=\"softmax\", name=ETHNICITY_OUT)(task_layers_ethnicity)\n",
    "    \n",
    "    task_layers_gender = layers.Dense(units=units, activation=\"relu\", name=\"gender_dense_1\")(shared_layers)\n",
    "    outputs_gender = layers.Dense(units=1, activation=\"sigmoid\", name=GENDER_OUT)(task_layers_gender)\n",
    "\n",
    "    # create the model\n",
    "    outputs_list = [outputs_age, outputs_ethnicity, outputs_gender]\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs_list, name=\"group_24_model_\" + str(id))\n",
    "    return model\n",
    "\n",
    "def make_session_dirs(parent_dir, session):\n",
    "    # folder to store the tensorboard logs\n",
    "    tensorlog_folder = os.path.join(parent_dir, \"logs\", str(session))\n",
    "    # folder to store the trained models\n",
    "    checkpoint_folder = os.path.join(parent_dir, \"models\", str(session))\n",
    "\n",
    "    Path(checkpoint_folder).mkdir(parents=True, exist_ok=True)\n",
    "    Path(tensorlog_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return checkpoint_folder, tensorlog_folder\n",
    "\n",
    "def get_callbacks(dir, session):\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    check_dir, tboard_dir = make_session_dirs(dir, session)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=check_dir,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False\n",
    "    )\n",
    "    tboard = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = tboard_dir\n",
    "    )\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.1,\n",
    "        patience=2,\n",
    "        min_lr=5e-5,\n",
    "        verbose=1\n",
    "    )\n",
    "    return [reduce_lr, early_stop, checkpoint, tboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_hparams(hparams, dir, session):\n",
    "    batch_size = hparams[HP_BATCH_SIZE]\n",
    "    itrs = get_df_itr(batch_size)\n",
    "\n",
    "    model = create_model(\n",
    "        filters=hparams[HP_FILTERS],\n",
    "        units=hparams[HP_UNITS],\n",
    "        id=session\n",
    "    )\n",
    "    model.compile(optimizer=hparams[HP_OPTIMIZER], loss=LOSSES, loss_weights=LOSS_WEIGHTS, metrics=METRICS)\n",
    "\n",
    "    callbacks = get_callbacks(dir, session)\n",
    "    \n",
    "    model.fit(\n",
    "        itrs[\"train\"],\n",
    "        validation_data=itrs[\"val\"],\n",
    "        steps_per_epoch=len(itrs[\"train\"]) / batch_size,\n",
    "        validation_steps=len(itrs[\"val\"]) / batch_size,\n",
    "        batch_size = batch_size,\n",
    "        epochs=20,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    test_evals = model.evaluate(itrs[\"test\"], verbose=1)\n",
    "    print(f\"Test evaluation:\\n{test_evals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Session 0------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'filters': 8, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\0\\assets\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.5900 - age_dense_out_loss: 9.9951 - ethnicity_dense_out_loss: 0.8417 - gender_dense_out_loss: 0.3183 - age_dense_out_mae: 9.9951 - ethnicity_dense_out_accuracy: 0.7112 - gender_dense_out_accuracy: 0.8629\n",
      "Test evaluation:\n",
      "[0.5899791717529297, 9.995105743408203, 0.8416990637779236, 0.3182695806026459, 9.995105743408203, 0.7112436294555664, 0.8628619909286499]\n",
      "\n",
      "------Session 1------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'filters': 8, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\1\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5492 - age_dense_out_loss: 9.5102 - ethnicity_dense_out_loss: 0.7757 - gender_dense_out_loss: 0.3037 - age_dense_out_mae: 9.5102 - ethnicity_dense_out_accuracy: 0.7315 - gender_dense_out_accuracy: 0.8680: 1s - loss: 0.5530 - age_dense_out_loss: 9.6358 - ethnicity_dense_out_loss: 0.7763 - gender_dense_out_loss: 0.3105 - age_dense_out_mae: 9.6358 - ethnicity_dense_out_accuracy: \n",
      "Test evaluation:\n",
      "[0.5491892099380493, 9.510218620300293, 0.7756836414337158, 0.30367380380630493, 9.510218620300293, 0.7314735651016235, 0.867972731590271]\n",
      "\n",
      "------Session 2------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'filters': 8, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\2\\assets\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5455 - age_dense_out_loss: 9.2734 - ethnicity_dense_out_loss: 0.7703 - gender_dense_out_loss: 0.3022 - age_dense_out_mae: 9.2734 - ethnicity_dense_out_accuracy: 0.7404 - gender_dense_out_accuracy: 0.8646\n",
      "Test evaluation:\n",
      "[0.5455005168914795, 9.27340030670166, 0.7702755928039551, 0.3021791875362396, 9.27340030670166, 0.7404173612594604, 0.8645656108856201]\n",
      "\n",
      "------Session 3------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'filters': 16, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\3\\assets\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5354 - age_dense_out_loss: 9.4097 - ethnicity_dense_out_loss: 0.7514 - gender_dense_out_loss: 0.3005 - age_dense_out_mae: 9.4097 - ethnicity_dense_out_accuracy: 0.7474 - gender_dense_out_accuracy: 0.8712\n",
      "Test evaluation:\n",
      "[0.535396158695221, 9.409653663635254, 0.7514497637748718, 0.3005233407020569, 9.409653663635254, 0.7474446296691895, 0.8711669445037842]\n",
      "\n",
      "------Session 4------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'filters': 16, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\4\\assets\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\4\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\4\\assets\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5601 - age_dense_out_loss: 9.4995 - ethnicity_dense_out_loss: 0.7899 - gender_dense_out_loss: 0.3113 - age_dense_out_mae: 9.4995 - ethnicity_dense_out_accuracy: 0.7279 - gender_dense_out_accuracy: 0.8665\n",
      "Test evaluation:\n",
      "[0.5601222515106201, 9.499504089355469, 0.7899448275566101, 0.311301052570343, 9.499504089355469, 0.7278534770011902, 0.866482138633728]\n",
      "\n",
      "------Session 5------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'filters': 16, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\5\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5317 - age_dense_out_loss: 8.6574 - ethnicity_dense_out_loss: 0.7485 - gender_dense_out_loss: 0.2975 - age_dense_out_mae: 8.6574 - ethnicity_dense_out_accuracy: 0.7434 - gender_dense_out_accuracy: 0.8720\n",
      "Test evaluation:\n",
      "[0.5316558480262756, 8.657439231872559, 0.748494029045105, 0.297503799200058, 8.657439231872559, 0.7433986663818359, 0.8720187544822693]\n",
      "\n",
      "------Session 6------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'filters': 32, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\6\\assets\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\6\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\6\\assets\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5798 - age_dense_out_loss: 9.9692 - ethnicity_dense_out_loss: 0.8090 - gender_dense_out_loss: 0.3307 - age_dense_out_mae: 9.9692 - ethnicity_dense_out_accuracy: 0.7219 - gender_dense_out_accuracy: 0.8580\n",
      "Test evaluation:\n",
      "[0.5798300504684448, 9.969220161437988, 0.8090434074401855, 0.33067819476127625, 9.969220161437988, 0.7218909859657288, 0.8579642176628113]\n",
      "\n",
      "------Session 7------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'filters': 32, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\7\\assets\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\7\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\7\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.5317 - age_dense_out_loss: 9.1617 - ethnicity_dense_out_loss: 0.7515 - gender_dense_out_loss: 0.2936 - age_dense_out_mae: 9.1617 - ethnicity_dense_out_accuracy: 0.7445 - gender_dense_out_accuracy: 0.8741\n",
      "Test evaluation:\n",
      "[0.5317260026931763, 9.161677360534668, 0.7515482902526855, 0.29358118772506714, 9.161677360534668, 0.7444633841514587, 0.8741481900215149]\n",
      "\n",
      "------Session 8------\n",
      "{'batch_size': 8, 'optimizer': 'Adam', 'filters': 32, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\8\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\8\\assets\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5457 - age_dense_out_loss: 10.0559 - ethnicity_dense_out_loss: 0.7779 - gender_dense_out_loss: 0.2935 - age_dense_out_mae: 10.0559 - ethnicity_dense_out_accuracy: 0.7451 - gender_dense_out_accuracy: 0.8716\n",
      "Test evaluation:\n",
      "[0.5457428097724915, 10.055903434753418, 0.7778810262680054, 0.2934938967227936, 10.055903434753418, 0.7451022267341614, 0.8715928196907043]\n",
      "\n",
      "------Session 9------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'filters': 8, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\9\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.6432 - age_dense_out_loss: 11.2963 - ethnicity_dense_out_loss: 0.9132 - gender_dense_out_loss: 0.3506 - age_dense_out_mae: 11.2963 - ethnicity_dense_out_accuracy: 0.6755 - gender_dense_out_accuracy: 0.8460\n",
      "Test evaluation:\n",
      "[0.6432003378868103, 11.296259880065918, 0.9131720066070557, 0.35063716769218445, 11.296259880065918, 0.6754685044288635, 0.8460391759872437]\n",
      "\n",
      "------Session 10------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'filters': 8, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\10\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.6298 - age_dense_out_loss: 11.2091 - ethnicity_dense_out_loss: 0.8799 - gender_dense_out_loss: 0.3573 - age_dense_out_mae: 11.2091 - ethnicity_dense_out_accuracy: 0.6938 - gender_dense_out_accuracy: 0.8409\n",
      "Test evaluation:\n",
      "[0.6298187375068665, 11.209089279174805, 0.8798741698265076, 0.357345849275589, 11.209089279174805, 0.6937819123268127, 0.8409284353256226]\n",
      "\n",
      "------Session 11------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'filters': 8, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\11\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.6061 - age_dense_out_loss: 12.2440 - ethnicity_dense_out_loss: 0.8446 - gender_dense_out_loss: 0.3430 - age_dense_out_mae: 12.2440 - ethnicity_dense_out_accuracy: 0.7087 - gender_dense_out_accuracy: 0.8492\n",
      "Test evaluation:\n",
      "[0.6060543656349182, 12.244022369384766, 0.8446191549301147, 0.34300172328948975, 12.244022369384766, 0.7086882591247559, 0.8492333889007568]\n",
      "\n",
      "------Session 12------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'filters': 16, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\12\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5939 - age_dense_out_loss: 10.3688 - ethnicity_dense_out_loss: 0.8364 - gender_dense_out_loss: 0.3307 - age_dense_out_mae: 10.3688 - ethnicity_dense_out_accuracy: 0.7115 - gender_dense_out_accuracy: 0.8597\n",
      "Test evaluation:\n",
      "[0.5939198732376099, 10.368769645690918, 0.8364409804344177, 0.3306616544723511, 10.368769645690918, 0.7114565372467041, 0.8596677780151367]\n",
      "\n",
      "------Session 13------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'filters': 16, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\13\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5715 - age_dense_out_loss: 10.3061 - ethnicity_dense_out_loss: 0.7961 - gender_dense_out_loss: 0.3263 - age_dense_out_mae: 10.3061 - ethnicity_dense_out_accuracy: 0.7240 - gender_dense_out_accuracy: 0.8586\n",
      "Test evaluation:\n",
      "[0.5715278387069702, 10.30614185333252, 0.796118974685669, 0.3263257145881653, 10.30614185333252, 0.7240204215049744, 0.8586030602455139]\n",
      "\n",
      "------Session 14------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'filters': 16, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\14\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5728 - age_dense_out_loss: 9.8338 - ethnicity_dense_out_loss: 0.7968 - gender_dense_out_loss: 0.3291 - age_dense_out_mae: 9.8338 - ethnicity_dense_out_accuracy: 0.7238 - gender_dense_out_accuracy: 0.8595: 1s - loss: 0.5693 - age_dense_out_loss: 9.5094 - ethnicity_dense_out_loss: 0.7966 - gender_dense_out_loss: 0.3229 - age_dense_out_mae: 9.5094 - ethn\n",
      "Test evaluation:\n",
      "[0.5727666020393372, 9.833847999572754, 0.7967527508735657, 0.3291133940219879, 9.833847999572754, 0.7238075137138367, 0.859454870223999]\n",
      "\n",
      "------Session 15------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'filters': 32, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\15\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5652 - age_dense_out_loss: 11.0975 - ethnicity_dense_out_loss: 0.7973 - gender_dense_out_loss: 0.3108 - age_dense_out_mae: 11.0975 - ethnicity_dense_out_accuracy: 0.7268 - gender_dense_out_accuracy: 0.8644\n",
      "Test evaluation:\n",
      "[0.5651522278785706, 11.097468376159668, 0.7972702980041504, 0.3108389377593994, 11.097468376159668, 0.7267887592315674, 0.8643526434898376]\n",
      "\n",
      "------Session 16------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'filters': 32, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\16\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5892 - age_dense_out_loss: 10.0457 - ethnicity_dense_out_loss: 0.8263 - gender_dense_out_loss: 0.3320 - age_dense_out_mae: 10.0457 - ethnicity_dense_out_accuracy: 0.7149 - gender_dense_out_accuracy: 0.8590\n",
      "Test evaluation:\n",
      "[0.5891823172569275, 10.04574203491211, 0.8263229131698608, 0.33195140957832336, 10.04574203491211, 0.7148637175559998, 0.8590289354324341]\n",
      "\n",
      "------Session 17------\n",
      "{'batch_size': 8, 'optimizer': 'Adamax', 'filters': 32, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\17\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5596 - age_dense_out_loss: 9.6672 - ethnicity_dense_out_loss: 0.7758 - gender_dense_out_loss: 0.3242 - age_dense_out_mae: 9.6672 - ethnicity_dense_out_accuracy: 0.7351 - gender_dense_out_accuracy: 0.8629\n",
      "Test evaluation:\n",
      "[0.5596417188644409, 9.667224884033203, 0.7757798433303833, 0.32416832447052, 9.667224884033203, 0.7350937128067017, 0.8628619909286499]\n",
      "\n",
      "------Session 18------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 8, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\18\\assets\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\18\\assets\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\18\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\18\\assets\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.5778 - age_dense_out_loss: 10.5178 - ethnicity_dense_out_loss: 0.8047 - gender_dense_out_loss: 0.3300 - age_dense_out_mae: 10.5178 - ethnicity_dense_out_accuracy: 0.7208 - gender_dense_out_accuracy: 0.8582\n",
      "Test evaluation:\n",
      "[0.5778446197509766, 10.517788887023926, 0.8046534061431885, 0.3299996852874756, 10.517788887023926, 0.7208262085914612, 0.8581771850585938]\n",
      "\n",
      "------Session 19------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 8, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\19\\assets\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\19\\assets\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\19\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\19\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5578 - age_dense_out_loss: 10.0525 - ethnicity_dense_out_loss: 0.7736 - gender_dense_out_loss: 0.3218 - age_dense_out_mae: 10.0525 - ethnicity_dense_out_accuracy: 0.7340 - gender_dense_out_accuracy: 0.8631\n",
      "Test evaluation:\n",
      "[0.5577550530433655, 10.052465438842773, 0.7736167311668396, 0.3217882215976715, 10.052465438842773, 0.7340289354324341, 0.8630749583244324]\n",
      "\n",
      "------Session 20------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 8, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\20\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5499 - age_dense_out_loss: 9.4826 - ethnicity_dense_out_loss: 0.7649 - gender_dense_out_loss: 0.3159 - age_dense_out_mae: 9.4826 - ethnicity_dense_out_accuracy: 0.7370 - gender_dense_out_accuracy: 0.8641\n",
      "Test evaluation:\n",
      "[0.5498591065406799, 9.482605934143066, 0.7648512721061707, 0.3159014880657196, 9.482605934143066, 0.7370102405548096, 0.8641396760940552]\n",
      "\n",
      "------Session 21------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 16, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\21\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\21\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\21\\assets\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\21\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5480 - age_dense_out_loss: 9.6808 - ethnicity_dense_out_loss: 0.7507 - gender_dense_out_loss: 0.3259 - age_dense_out_mae: 9.6808 - ethnicity_dense_out_accuracy: 0.7496 - gender_dense_out_accuracy: 0.8607\n",
      "Test evaluation:\n",
      "[0.5479597449302673, 9.68081283569336, 0.7506890892982483, 0.32586854696273804, 9.68081283569336, 0.7495741248130798, 0.8607325553894043]\n",
      "\n",
      "------Session 22------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 16, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\22\\assets\n",
      "587/587 [==============================] - 2s 4ms/step - loss: 0.5607 - age_dense_out_loss: 10.0566 - ethnicity_dense_out_loss: 0.7769 - gender_dense_out_loss: 0.3244 - age_dense_out_mae: 10.0566 - ethnicity_dense_out_accuracy: 0.7349 - gender_dense_out_accuracy: 0.8626\n",
      "Test evaluation:\n",
      "[0.560697078704834, 10.056597709655762, 0.7768646478652954, 0.32441699504852295, 10.056597709655762, 0.7348807454109192, 0.8626490831375122]\n",
      "\n",
      "------Session 23------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 16, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\23\\assets\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.5406 - age_dense_out_loss: 8.8028 - ethnicity_dense_out_loss: 0.7528 - gender_dense_out_loss: 0.3108 - age_dense_out_mae: 8.8028 - ethnicity_dense_out_accuracy: 0.7432 - gender_dense_out_accuracy: 0.8682\n",
      "Test evaluation:\n",
      "[0.5405654907226562, 8.8027925491333, 0.7527714371681213, 0.3107545077800751, 8.8027925491333, 0.7431856989860535, 0.8681856989860535]\n",
      "\n",
      "------Session 24------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 32, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\24\\assets\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.5446 - age_dense_out_loss: 9.7519 - ethnicity_dense_out_loss: 0.7637 - gender_dense_out_loss: 0.3060 - age_dense_out_mae: 9.7519 - ethnicity_dense_out_accuracy: 0.7434 - gender_dense_out_accuracy: 0.8705\n",
      "Test evaluation:\n",
      "[0.5445753931999207, 9.751923561096191, 0.7636716365814209, 0.30597442388534546, 9.751923561096191, 0.7433986663818359, 0.8705281019210815]\n",
      "\n",
      "------Session 25------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 32, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\25\\assets\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\25\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\25\\assets\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\25\\assets\n",
      "587/587 [==============================] - 2s 3ms/step - loss: 0.5413 - age_dense_out_loss: 9.2588 - ethnicity_dense_out_loss: 0.7626 - gender_dense_out_loss: 0.3014 - age_dense_out_mae: 9.2588 - ethnicity_dense_out_accuracy: 0.7408 - gender_dense_out_accuracy: 0.8712\n",
      "Test evaluation:\n",
      "[0.5412631034851074, 9.258784294128418, 0.7626006603240967, 0.30140677094459534, 9.258784294128418, 0.7408432960510254, 0.8711669445037842]\n",
      "\n",
      "------Session 26------\n",
      "{'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 32, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\26\\assets\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\26\\assets\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\26\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\26\\assets\n",
      "587/587 [==============================] - 3s 4ms/step - loss: 0.5434 - age_dense_out_loss: 9.4171 - ethnicity_dense_out_loss: 0.7538 - gender_dense_out_loss: 0.3141 - age_dense_out_mae: 9.4171 - ethnicity_dense_out_accuracy: 0.7447 - gender_dense_out_accuracy: 0.8624\n",
      "Test evaluation:\n",
      "[0.5433643460273743, 9.417086601257324, 0.7538124918937683, 0.31408172845840454, 9.417086601257324, 0.7446762919425964, 0.8624361157417297]\n",
      "\n",
      "------Session 27------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'filters': 8, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\27\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6930 - age_dense_out_loss: 11.5552 - ethnicity_dense_out_loss: 0.9477 - gender_dense_out_loss: 0.4152 - age_dense_out_mae: 11.5552 - ethnicity_dense_out_accuracy: 0.6603 - gender_dense_out_accuracy: 0.8052\n",
      "Test evaluation:\n",
      "[0.6930127143859863, 11.555154800415039, 0.9477227330207825, 0.41519230604171753, 11.555154800415039, 0.6603492498397827, 0.8051533102989197]\n",
      "\n",
      "------Session 28------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'filters': 8, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\28\\assets\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6080 - age_dense_out_loss: 11.4215 - ethnicity_dense_out_loss: 0.8502 - gender_dense_out_loss: 0.3428 - age_dense_out_mae: 11.4215 - ethnicity_dense_out_accuracy: 0.6995 - gender_dense_out_accuracy: 0.8550\n",
      "Test evaluation:\n",
      "[0.6079610586166382, 11.421502113342285, 0.850234866142273, 0.34284403920173645, 11.421502113342285, 0.6995314955711365, 0.8549829721450806]\n",
      "\n",
      "------Session 29------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'filters': 8, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\29\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5970 - age_dense_out_loss: 10.6967 - ethnicity_dense_out_loss: 0.8292 - gender_dense_out_loss: 0.3435 - age_dense_out_mae: 10.6967 - ethnicity_dense_out_accuracy: 0.7119 - gender_dense_out_accuracy: 0.8522\n",
      "Test evaluation:\n",
      "[0.5970310568809509, 10.69672679901123, 0.8291893005371094, 0.3434796631336212, 10.69672679901123, 0.711882472038269, 0.8522146344184875]\n",
      "\n",
      "------Session 30------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'filters': 16, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\30\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5866 - age_dense_out_loss: 10.9682 - ethnicity_dense_out_loss: 0.8089 - gender_dense_out_loss: 0.3424 - age_dense_out_mae: 10.9682 - ethnicity_dense_out_accuracy: 0.7176 - gender_dense_out_accuracy: 0.8503\n",
      "Test evaluation:\n",
      "[0.5866463780403137, 10.968170166015625, 0.8089438676834106, 0.3424128293991089, 10.968170166015625, 0.7176320552825928, 0.8502981066703796]\n",
      "\n",
      "------Session 31------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'filters': 16, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\31\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5780 - age_dense_out_loss: 10.7685 - ethnicity_dense_out_loss: 0.8124 - gender_dense_out_loss: 0.3220 - age_dense_out_mae: 10.7685 - ethnicity_dense_out_accuracy: 0.7166 - gender_dense_out_accuracy: 0.8586\n",
      "Test evaluation:\n",
      "[0.5779738426208496, 10.768482208251953, 0.8124391436576843, 0.3219720423221588, 10.768482208251953, 0.7165672779083252, 0.8586030602455139]\n",
      "\n",
      "------Session 32------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'filters': 16, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\32\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5849 - age_dense_out_loss: 10.6912 - ethnicity_dense_out_loss: 0.8114 - gender_dense_out_loss: 0.3371 - age_dense_out_mae: 10.6912 - ethnicity_dense_out_accuracy: 0.7204 - gender_dense_out_accuracy: 0.8535\n",
      "Test evaluation:\n",
      "[0.5849473476409912, 10.69116497039795, 0.8114290237426758, 0.3370828628540039, 10.69116497039795, 0.720400333404541, 0.8534923195838928]\n",
      "\n",
      "------Session 33------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'filters': 32, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\33\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5885 - age_dense_out_loss: 9.9351 - ethnicity_dense_out_loss: 0.8268 - gender_dense_out_loss: 0.3305 - age_dense_out_mae: 9.9351 - ethnicity_dense_out_accuracy: 0.7066 - gender_dense_out_accuracy: 0.8571\n",
      "Test evaluation:\n",
      "[0.5885477066040039, 9.935126304626465, 0.8267633318901062, 0.33046162128448486, 9.935126304626465, 0.7065587639808655, 0.8571124076843262]\n",
      "\n",
      "------Session 34------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'filters': 32, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\34\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5842 - age_dense_out_loss: 10.5343 - ethnicity_dense_out_loss: 0.8109 - gender_dense_out_loss: 0.3364 - age_dense_out_mae: 10.5343 - ethnicity_dense_out_accuracy: 0.7178 - gender_dense_out_accuracy: 0.8516\n",
      "Test evaluation:\n",
      "[0.5841819643974304, 10.534299850463867, 0.8108544945716858, 0.3364415466785431, 10.534299850463867, 0.7178449630737305, 0.8515757918357849]\n",
      "\n",
      "------Session 35------\n",
      "{'batch_size': 16, 'optimizer': 'Adam', 'filters': 32, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\35\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\35\\assets\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6028 - age_dense_out_loss: 10.9695 - ethnicity_dense_out_loss: 0.8568 - gender_dense_out_loss: 0.3269 - age_dense_out_mae: 10.9695 - ethnicity_dense_out_accuracy: 0.7027 - gender_dense_out_accuracy: 0.8597\n",
      "Test evaluation:\n",
      "[0.602820634841919, 10.969478607177734, 0.8567950129508972, 0.32690730690956116, 10.969478607177734, 0.7027257084846497, 0.8596677780151367]\n",
      "\n",
      "------Session 36------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'filters': 8, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\36\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.7818 - age_dense_out_loss: 13.0790 - ethnicity_dense_out_loss: 1.0626 - gender_dense_out_loss: 0.4748 - age_dense_out_mae: 13.0790 - ethnicity_dense_out_accuracy: 0.6380 - gender_dense_out_accuracy: 0.7845\n",
      "Test evaluation:\n",
      "[0.7818126678466797, 13.079012870788574, 1.0626200437545776, 0.4748465418815613, 13.079012870788574, 0.6379897594451904, 0.7844974398612976]\n",
      "\n",
      "------Session 37------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'filters': 8, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\37\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.7650 - age_dense_out_loss: 13.6329 - ethnicity_dense_out_loss: 1.0475 - gender_dense_out_loss: 0.4552 - age_dense_out_mae: 13.6329 - ethnicity_dense_out_accuracy: 0.6282 - gender_dense_out_accuracy: 0.7911\n",
      "Test evaluation:\n",
      "[0.7650042772293091, 13.632899284362793, 1.047541856765747, 0.45520108938217163, 13.632899284362793, 0.6281942129135132, 0.7910988330841064]\n",
      "\n",
      "------Session 38------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'filters': 8, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\38\\assets\n",
      "294/294 [==============================] - 1s 5ms/step - loss: 0.6805 - age_dense_out_loss: 12.2343 - ethnicity_dense_out_loss: 0.9593 - gender_dense_out_loss: 0.3773 - age_dense_out_mae: 12.2343 - ethnicity_dense_out_accuracy: 0.6657 - gender_dense_out_accuracy: 0.8362\n",
      "Test evaluation:\n",
      "[0.6805230379104614, 12.23434829711914, 0.9592634439468384, 0.377314031124115, 12.23434829711914, 0.6656728982925415, 0.8362436294555664]\n",
      "\n",
      "------Session 39------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'filters': 16, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\39\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6962 - age_dense_out_loss: 12.8305 - ethnicity_dense_out_loss: 0.9751 - gender_dense_out_loss: 0.3916 - age_dense_out_mae: 12.8305 - ethnicity_dense_out_accuracy: 0.6527 - gender_dense_out_accuracy: 0.8262\n",
      "Test evaluation:\n",
      "[0.696169376373291, 12.830535888671875, 0.9751225113868713, 0.3915560841560364, 12.830535888671875, 0.6526831388473511, 0.8262351155281067]\n",
      "\n",
      "------Session 40------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'filters': 16, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\40\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6676 - age_dense_out_loss: 11.7680 - ethnicity_dense_out_loss: 0.9429 - gender_dense_out_loss: 0.3689 - age_dense_out_mae: 11.7680 - ethnicity_dense_out_accuracy: 0.6606 - gender_dense_out_accuracy: 0.8384\n",
      "Test evaluation:\n",
      "[0.6676377058029175, 11.7680025100708, 0.9428709149360657, 0.3688691556453705, 11.7680025100708, 0.6605621576309204, 0.838373064994812]\n",
      "\n",
      "------Session 41------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'filters': 16, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\41\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6769 - age_dense_out_loss: 12.3057 - ethnicity_dense_out_loss: 0.9374 - gender_dense_out_loss: 0.3918 - age_dense_out_mae: 12.3057 - ethnicity_dense_out_accuracy: 0.6714 - gender_dense_out_accuracy: 0.8328\n",
      "Test evaluation:\n",
      "[0.6769235134124756, 12.30570125579834, 0.9374184608459473, 0.3918161690235138, 12.30570125579834, 0.6714224815368652, 0.8328364491462708]\n",
      "\n",
      "------Session 42------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'filters': 32, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\42\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6361 - age_dense_out_loss: 12.0508 - ethnicity_dense_out_loss: 0.8988 - gender_dense_out_loss: 0.3492 - age_dense_out_mae: 12.0508 - ethnicity_dense_out_accuracy: 0.6802 - gender_dense_out_accuracy: 0.8529\n",
      "Test evaluation:\n",
      "[0.6360934972763062, 12.050780296325684, 0.8988472819328308, 0.3492380678653717, 12.050780296325684, 0.6801533102989197, 0.8528534770011902]\n",
      "\n",
      "------Session 43------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'filters': 32, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\43\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6574 - age_dense_out_loss: 11.7336 - ethnicity_dense_out_loss: 0.9314 - gender_dense_out_loss: 0.3600 - age_dense_out_mae: 11.7336 - ethnicity_dense_out_accuracy: 0.6816 - gender_dense_out_accuracy: 0.8426\n",
      "Test evaluation:\n",
      "[0.6574369668960571, 11.733552932739258, 0.9314033389091492, 0.36000365018844604, 11.733552932739258, 0.6816439628601074, 0.8426320552825928]\n",
      "\n",
      "------Session 44------\n",
      "{'batch_size': 16, 'optimizer': 'Adamax', 'filters': 32, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\44\\assets\n",
      "294/294 [==============================] - 1s 5ms/step - loss: 0.6331 - age_dense_out_loss: 11.2542 - ethnicity_dense_out_loss: 0.8763 - gender_dense_out_loss: 0.3675 - age_dense_out_mae: 11.2542 - ethnicity_dense_out_accuracy: 0.6946 - gender_dense_out_accuracy: 0.8467\n",
      "Test evaluation:\n",
      "[0.6331373453140259, 11.254169464111328, 0.8762811422348022, 0.36748525500297546, 11.254169464111328, 0.6946337223052979, 0.8466780185699463]\n",
      "\n",
      "------Session 45------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'filters': 8, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\45\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6204 - age_dense_out_loss: 11.0874 - ethnicity_dense_out_loss: 0.8860 - gender_dense_out_loss: 0.3325 - age_dense_out_mae: 11.0874 - ethnicity_dense_out_accuracy: 0.6851 - gender_dense_out_accuracy: 0.8567\n",
      "Test evaluation:\n",
      "[0.6203728914260864, 11.08737564086914, 0.8860480785369873, 0.33252260088920593, 11.08737564086914, 0.6850510835647583, 0.856686532497406]\n",
      "\n",
      "------Session 46------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'filters': 8, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\46\\assets\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\46\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\46\\assets\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\46\\assets\n",
      "294/294 [==============================] - 1s 5ms/step - loss: 0.5841 - age_dense_out_loss: 11.0332 - ethnicity_dense_out_loss: 0.8085 - gender_dense_out_loss: 0.3376 - age_dense_out_mae: 11.0332 - ethnicity_dense_out_accuracy: 0.7164 - gender_dense_out_accuracy: 0.8526\n",
      "Test evaluation:\n",
      "[0.5840849280357361, 11.033159255981445, 0.8085097670555115, 0.3375926911830902, 11.033159255981445, 0.7163543701171875, 0.8526405692100525]\n",
      "\n",
      "------Session 47------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'filters': 8, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\47\\assets\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\47\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\47\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\47\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6124 - age_dense_out_loss: 11.2616 - ethnicity_dense_out_loss: 0.8606 - gender_dense_out_loss: 0.3418 - age_dense_out_mae: 11.2616 - ethnicity_dense_out_accuracy: 0.7019 - gender_dense_out_accuracy: 0.8524\n",
      "Test evaluation:\n",
      "[0.6124456524848938, 11.261645317077637, 0.8605580925941467, 0.3418106436729431, 11.261645317077637, 0.7018739581108093, 0.85242760181427]\n",
      "\n",
      "------Session 48------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'filters': 16, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\48\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.6354 - age_dense_out_loss: 11.4814 - ethnicity_dense_out_loss: 0.8992 - gender_dense_out_loss: 0.3486 - age_dense_out_mae: 11.4814 - ethnicity_dense_out_accuracy: 0.6810 - gender_dense_out_accuracy: 0.8439\n",
      "Test evaluation:\n",
      "[0.6353845000267029, 11.481390953063965, 0.8991690874099731, 0.34863656759262085, 11.481390953063965, 0.6810051202774048, 0.8439096808433533]\n",
      "\n",
      "------Session 49------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'filters': 16, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\49\\assets\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5873 - age_dense_out_loss: 10.4873 - ethnicity_dense_out_loss: 0.8180 - gender_dense_out_loss: 0.3357 - age_dense_out_mae: 10.4873 - ethnicity_dense_out_accuracy: 0.7198 - gender_dense_out_accuracy: 0.8556\n",
      "Test evaluation:\n",
      "[0.5873394012451172, 10.487327575683594, 0.8179547190666199, 0.3357495367527008, 10.487327575683594, 0.7197614908218384, 0.8556218147277832]\n",
      "\n",
      "------Session 50------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'filters': 16, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\50\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5718 - age_dense_out_loss: 10.0277 - ethnicity_dense_out_loss: 0.8005 - gender_dense_out_loss: 0.3229 - age_dense_out_mae: 10.0277 - ethnicity_dense_out_accuracy: 0.7223 - gender_dense_out_accuracy: 0.8578\n",
      "Test evaluation:\n",
      "[0.5717709064483643, 10.027737617492676, 0.8005422949790955, 0.3229438364505768, 10.027737617492676, 0.7223168611526489, 0.8577512502670288]\n",
      "\n",
      "------Session 51------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'filters': 32, 'units': 64}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\51\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5882 - age_dense_out_loss: 10.8545 - ethnicity_dense_out_loss: 0.8267 - gender_dense_out_loss: 0.3279 - age_dense_out_mae: 10.8545 - ethnicity_dense_out_accuracy: 0.7134 - gender_dense_out_accuracy: 0.8573\n",
      "Test evaluation:\n",
      "[0.5881799459457397, 10.854521751403809, 0.8267245888710022, 0.3279266655445099, 10.854521751403809, 0.713373064994812, 0.8573253750801086]\n",
      "\n",
      "------Session 52------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'filters': 32, 'units': 128}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\52\\assets\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5734 - age_dense_out_loss: 10.2620 - ethnicity_dense_out_loss: 0.8002 - gender_dense_out_loss: 0.3262 - age_dense_out_mae: 10.2620 - ethnicity_dense_out_accuracy: 0.7293 - gender_dense_out_accuracy: 0.8590\n",
      "Test evaluation:\n",
      "[0.5734359622001648, 10.262002944946289, 0.800150454044342, 0.32619717717170715, 10.262002944946289, 0.7293441295623779, 0.8590289354324341]\n",
      "\n",
      "------Session 53------\n",
      "{'batch_size': 16, 'optimizer': 'RMSprop', 'filters': 32, 'units': 256}\n",
      "Found 15026 validated image filenames.\n",
      "Found 3757 validated image filenames.\n",
      "Found 4696 validated image filenames.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "INFO:tensorflow:Assets written to: .\\fine_tune\\20220401_223058\\models\\53\\assets\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 5e-05.\n",
      "294/294 [==============================] - 1s 4ms/step - loss: 0.5743 - age_dense_out_loss: 10.1142 - ethnicity_dense_out_loss: 0.8006 - gender_dense_out_loss: 0.3278 - age_dense_out_mae: 10.1142 - ethnicity_dense_out_accuracy: 0.7274 - gender_dense_out_accuracy: 0.8588\n",
      "Test evaluation:\n",
      "[0.5743213891983032, 10.114235877990723, 0.8006253838539124, 0.3277888596057892, 10.114235877990723, 0.72742760181427, 0.8588160276412964]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_fine_tune_dirs():\n",
    "    timestamp = datetime.datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "    dir = os.path.join(os.path.curdir, \"fine_tune\", timestamp)\n",
    "    Path(dir).mkdir(parents=True, exist_ok=True)\n",
    "    return dir\n",
    "\n",
    "dir = make_fine_tune_dirs()\n",
    "session = 0\n",
    "for bs in HP_BATCH_SIZE.domain.values:\n",
    "    for op in HP_OPTIMIZER.domain.values:\n",
    "        for ft in HP_FILTERS.domain.values:\n",
    "            for units in HP_UNITS.domain.values:\n",
    "                print(f\"------Session {session}------\")\n",
    "                hparams = {\n",
    "                    HP_BATCH_SIZE: bs,\n",
    "                    HP_OPTIMIZER: op,\n",
    "                    HP_FILTERS: ft,\n",
    "                    HP_UNITS: units\n",
    "                }\n",
    "                print({h.name: hparams[h] for h in hparams})\n",
    "                fine_tune_hparams(hparams, dir, session)\n",
    "                session += 1\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present your findings for 5 different runs by fine-tuning the hyperparameters. The results table must contain the following fields\n",
    "- A minimum of 5 hyperparameters that you have fine-tuned\n",
    "- Mean absolute error for age\n",
    "- Accuracy for ethnicity prediction\n",
    "- Accuracy for gender prediction\n",
    "Please use a table format similar to the one mentioned below when presenting the results.\n",
    "\n",
    "With the `Session id` below you can find the tensorboard logs and models in `./fine_tune/<time_stamp>` dir\n",
    "\n",
    "|Session id| Hyperparameters | Age(MAE) | Ethnicity(Accuracy)| Gender(Accuracy)|\n",
    "|-------|-----------------|----------|--------------------|------------------|\n",
    "| 7 | {'batch_size': 8, 'optimizer': 'Adam', 'filters': 32, 'units': 128} | 9.1617 | 0.7445 | 0.8741 |\n",
    "| 23 | {'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 16, 'units': 256} | 8.8028 | 0.7432 | 0.8682 |\n",
    "| 24 | {'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 32, 'units': 64} | 9.7519 | 0.7434 | 0.8705 |\n",
    "| 20 | {'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 8, 'units': 256} | 9.4826 | 0.7370 | 0.8641 |\n",
    "| 26 | {'batch_size': 8, 'optimizer': 'RMSprop', 'filters': 32, 'units': 256} | 9.4171 | 0.7447 | 0.8624 |\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcacb1169d0317e97b762e0c1a317633c556cb59cb1017287d7e24b96b766e04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('csi4106')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
